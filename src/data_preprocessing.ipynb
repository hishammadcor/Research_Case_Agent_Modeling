{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicate_cols(file_path):\n",
    "    '''\n",
    "    Input: csv file path\n",
    "    Checks if the csv file have duplicate column names.\n",
    "    '''\n",
    "    df = pd.read_csv(file_path,encoding=\"utf-8\")\n",
    "\n",
    "    # Get column names and find duplicates\n",
    "    column_names = df.columns\n",
    "    duplicate_columns = column_names[column_names.duplicated()].tolist()\n",
    "\n",
    "    if duplicate_columns:\n",
    "        print(f\"WARNING! Duplicate column titles found: {duplicate_columns}\")\n",
    "    else:\n",
    "        print(\"\\nNice! No duplicate column titles found.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns_from_formatted_codebook(file_path_formatted_codebook, verbose):\n",
    "    '''\n",
    "    input: formatted codebook filepath\n",
    "    Read the formatted codebook csv to get the questions and their color categories\n",
    "    '''\n",
    "    # Read the dataset from file\n",
    "    df = pd.read_csv(file_path_formatted_codebook)\n",
    "\n",
    "    # Creating no_color_questions for rows where 'color' is empty\n",
    "    no_color_questions = df.loc[df['Color_Category'].isnull(), 'Custom_variable_name'].tolist()\n",
    "\n",
    "    # Creating lists for each color, where grey_questions=question having sub categories.\n",
    "    grey_questions  = df.loc[df['Color_Category'] == 'Grey', 'Custom_variable_name'].tolist()\n",
    "    yellow_questions = df.loc[df['Color_Category'] == 'Yellow', 'Custom_variable_name'].tolist()\n",
    "    green_questions  = df.loc[df['Color_Category'] == 'Green', 'Custom_variable_name'].tolist()\n",
    "\n",
    "    # Output the results if needed\n",
    "    if(verbose):\n",
    "        print(\"::::get_columns_from_formatted_codebook:::::Returns\")\n",
    "        print(\"no_color_questions:\", no_color_questions)\n",
    "        print(\"grey_questions(question having sub categories):\", grey_questions )\n",
    "        print(\"yellow_questions:\", yellow_questions )\n",
    "        print(\"green_questions:\", green_questions )\n",
    "    return no_color_questions, grey_questions,yellow_questions, green_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_datasets_for_required_col(file_path_dataset,file_path_formatted_sosec_code_book, columns_to_keep, verbose=False):\n",
    "    '''\n",
    "    input:codebook file path and sosec dataset file path\n",
    "    Only Keeps the columns in the dataset which are also in the codebook and saves the files\n",
    "    also Saves a file for the columns in the code book which are not in the dataset\n",
    "    '''\n",
    "    # Read the dataset CSV file\n",
    "    df = pd.read_csv(file_path_dataset)\n",
    "\n",
    "    # Check which columns are not found\n",
    "    not_found_columns = [col for col in columns_to_keep if col not in df.columns]\n",
    "\n",
    "    if not_found_columns:\n",
    "        # Read the formatted_sosec_code_book CSV file\n",
    "        file_path_2 = file_path_formatted_sosec_code_book\n",
    "        df2 = pd.read_csv(file_path_2,encoding=\"utf-8\")\n",
    "        \n",
    "        # Filter rows where Custom_variable_name matches values in not_found_columns\n",
    "        df_no_matches = df2[df2['Custom_variable_name'].isin(not_found_columns)]\n",
    "        \n",
    "        # Select the corresponding Text values\n",
    "        result_df = df_no_matches[['Custom_variable_name', 'Text']]\n",
    "        \n",
    "        # Save the result as a new CSV\n",
    "        output_file_path = r'../data/1_preprocess/1_codebook_no_matching_columns_in_dataset.csv'\n",
    "        result_df.to_csv(output_file_path, index=False)\n",
    "        if(verbose):\n",
    "            print(f\"Not Matched columns saved to: {output_file_path}\")\n",
    "\n",
    "    # Filter the DataFrame to keep only the columns that exist in the DataFrame\n",
    "    df_filtered = df[[col for col in columns_to_keep if col in df.columns]]\n",
    "    \n",
    "    output_file_path = r'../data/1_preprocess/1_df_dataset_with_codebook_columns_full_no_processing.csv'\n",
    "    df_filtered.to_csv(output_file_path, index=False)\n",
    "\n",
    "    if(verbose):\n",
    "        print(df_filtered.head())\n",
    "    \n",
    "    return output_file_path+\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row_count(file_path_csv):\n",
    "    '''\n",
    "    input filepath of csv\n",
    "    Returns the number of rows in the given CSV file.\n",
    "    '''\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path_csv)\n",
    "\n",
    "    # Get the number of rows\n",
    "    row_count = len(df)\n",
    "\n",
    "    print(f\"Number of rows in the csv: {row_count}\")\n",
    "    return row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_columns_in_csv(file_path_csv):\n",
    "    \"\"\"\n",
    "    Counts and returns the number of columns in a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): The path to the CSV file.\n",
    "    \n",
    "    Returns:\n",
    "    int: The number of columns in the CSV file.\n",
    "    \"\"\"\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path_csv)\n",
    "    \n",
    "    # Get the number of col\n",
    "    columns_count = len(df.columns)\n",
    "    \n",
    "    \n",
    "    print(f\"Number of columns in the CSV: {columns_count}\")\n",
    "    # Return the number of columns\n",
    "    return columns_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_filter_on_dataset_F2A3(file_path_dataset_with_codebook_columns):\n",
    "    '''\n",
    "    input the datafile path with only codebook columns\n",
    "    replace 6 and 0 by blank\n",
    "    F2A3 = The possibility of losing your job (leave this empty if you do not work)\n",
    "    '''\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path_dataset_with_codebook_columns)\n",
    "\n",
    "    # Replace 6 and 0 in the 'F2A3' column with  empty\n",
    "    df['F2A3'] = df['F2A3'].replace({6: \"\", 0: \"\"})\n",
    "\n",
    "    # Save the updated DataFrame back to a CSV file\n",
    "    output_file_path = r'../data/1_preprocess/2_df_dataset_with_codebook_columns_full_F2A3.csv'\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Updated dataset saved to: {output_file_path}\")\n",
    "    return output_file_path+\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_filter_on_dataset_F7mA1(file_path_dataset_with_codebook_columns):\n",
    "    '''\n",
    "    input the datafile path with only codebook columns\n",
    "    replace 0 by blank\n",
    "    F7mA1 = job category (leave this empty if you do not work)\n",
    "    '''\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path_dataset_with_codebook_columns)\n",
    "\n",
    "    # Replace 0 in the 'F7mA1' column with  empty\n",
    "    df['F7mA1'] = df['F7mA1'].replace({0: \"\"})\n",
    "\n",
    "    # Save the updated DataFrame back to a CSV file\n",
    "    output_file_path = r'../data/1_preprocess/3_df_dataset_with_codebook_columns_full_F7mA1.csv'\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Updated dataset saved to: {output_file_path}\")\n",
    "    return output_file_path+\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_rows_for_out_of_range_data(sosec_data_path,file_path_dataset_with_codebook_columns,verbose=False):\n",
    "    '''\n",
    "    input: sosec_datafile path and sosec dataset with only codebook columns\n",
    "    Deletes the rows for all the columns for with the values are out of range. and saves the csv\n",
    "    '''\n",
    "    file1 = pd.read_csv(sosec_data_path)\n",
    "\n",
    "    file2 = pd.read_csv(file_path_dataset_with_codebook_columns)\n",
    "\n",
    "    # Create a dictionary from file2 with ranges\n",
    "    ranges = {}\n",
    "    for _, row in file2.iterrows():\n",
    "        col = row['Custom_variable_name']\n",
    "        range_str = row['Characteristic']\n",
    "        \n",
    "        # Check if range_str is a valid string before splitting\n",
    "        if isinstance(range_str, str) and range_str and range_str != 'F':  # Valid range and not 'F'\n",
    "            # If there's a valid range, split it into a list of integers\n",
    "            ranges[col] = list(map(int, range_str.split(',')))\n",
    "        else:\n",
    "            # If no valid range is provided (empty or 'F'), set the range to None\n",
    "            ranges[col] = None\n",
    "\n",
    "\n",
    "    # Validate the data in file1 against the ranges\n",
    "    def validate_data(file1, ranges):\n",
    "        errors = []\n",
    "        valid_rows = file1.copy()  # Copy of the original DataFrame to modify\n",
    "        \n",
    "        # Loop through each column and validate values\n",
    "        for col in file1.columns:\n",
    "            if col in ranges:\n",
    "                valid_range = ranges[col]\n",
    "                if valid_range is not None:  # Only check if a valid range exists\n",
    "                    # Create a boolean mask for invalid rows\n",
    "                    invalid_rows = ~(valid_rows[col].isin(valid_range) | valid_rows[col].isna())      \n",
    "\n",
    "                    # Track errors for rows with out-of-range data\n",
    "                    for index, value in valid_rows[invalid_rows][col].dropna().items():\n",
    "                        errors.append(f\"Out of range: {col} at row {index + 1} with value {value}\")\n",
    "                    \n",
    "                    # Remove rows with invalid data\n",
    "                    valid_rows = valid_rows[~invalid_rows]\n",
    "                else:\n",
    "                    # If no range is provided, assume all values are valid for that column\n",
    "                    continue\n",
    "        \n",
    "        return valid_rows, errors\n",
    "\n",
    "    # Get valid rows and errors\n",
    "    valid_rows, errors = validate_data(file1, ranges)\n",
    "\n",
    "    # Save the valid rows to a CSV file\n",
    "    output_file_path = r\"../data/1_preprocess/4_df_dataset_with_codebook_columns_filtered_outofrange.csv\"\n",
    "    valid_rows.to_csv(output_file_path, index=False)\n",
    "\n",
    "    if(verbose):\n",
    "        # Output validation errors\n",
    "        if errors:\n",
    "            print(\"Validation Errors:\")\n",
    "            for error in errors:\n",
    "                print(error)\n",
    "        else:\n",
    "            print(\"All data is within valid ranges.\")\n",
    "\n",
    "    # Print where the cleaned data has been saved\n",
    "    print(f\"Cleaned data saved to '{output_file_path}'.\")\n",
    "\n",
    "    return output_file_path+\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_filter_on_dataset_F7cA1(file_path_dataset_with_codebook_columns,min_yob,max_yob):\n",
    "    '''\n",
    "    filter rows based on range for F7cA1(Yob).\n",
    "    '''\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path_dataset_with_codebook_columns)\n",
    "\n",
    "    # Filter out rows where 'F7cA1' is not in range\n",
    "    df = df[(df['F7cA1'] >= min_yob) & (df['F7cA1'] <= max_yob)]\n",
    "\n",
    "    # Save the updated DataFrame back to a CSV file\n",
    "    output_file_path = r'../data/1_preprocess/5_df_dataset_with_codebook_columns_filtered_F7cA1-yob.csv'\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Updated dataset saved to: {output_file_path}\")\n",
    "    return output_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_high_null_columns(FILE_PATH_DATASET,file_path_formatted_sosec_code_book ,col_to_exclude, threshold=0.7 ):\n",
    "    '''\n",
    "    Deletes columns with more than 70% null values from file_name_A, excluding from list of columns and saves the dataset.\n",
    "    Prints removed columns and saves them to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        file_name_A (str): Path to the input CSV file.\n",
    "        col_to_exclude(list): Name of the column to exclude from deletion.\n",
    "        threshold (float): Proportion of nulls above which columns are dropped.\n",
    "        output_file (str): Path to save the filtered dataset.\n",
    "        removed_columns_file (str): Path to save the list of removed columns.\n",
    "    '''\n",
    "\n",
    "    output_file = r\"../data/1_preprocess/6_df_dataset_with_codebook_columns_filtered_lessdata.csv\"\n",
    "\n",
    "    # Load the dataset\n",
    "    df_A = pd.read_csv(FILE_PATH_DATASET)\n",
    "    \n",
    "    # Calculate the threshold for null values\n",
    "    null_threshold = threshold * len(df_A)\n",
    "    \n",
    "    # Identify columns to keep based on null percentage and exceptions\n",
    "    cols_to_keep = [col for col in df_A.columns \n",
    "                    if (df_A[col].isna().sum() <= null_threshold) or col in col_to_exclude]\n",
    "    \n",
    "    # Identify the columns to remove\n",
    "    cols_to_remove = [col for col in df_A.columns if col not in cols_to_keep]\n",
    "    \n",
    "    # Print the removed columns\n",
    "    print(f\"Removed columns: {cols_to_remove}\")\n",
    "    \n",
    "\n",
    "    if cols_to_remove:\n",
    "        # Read the formatted_sosec_code_book CSV file\n",
    "        file_path_2 = file_path_formatted_sosec_code_book\n",
    "        df2 = pd.read_csv(file_path_2,encoding=\"utf-8\")\n",
    "        \n",
    "        # Filter rows where Custom_variable_name matches values in not_found_columns\n",
    "        df_no_matches = df2[df2['Custom_variable_name'].isin(cols_to_remove)]\n",
    "        \n",
    "        # Select the corresponding Text values\n",
    "        result_df = df_no_matches[['Custom_variable_name', 'Text']]\n",
    "        \n",
    "        # Save the result as a new CSV\n",
    "        removed_columns_file= r\"../data/1_preprocess/6_removed_columns_due_to_lessdata.csv\"\n",
    "        result_df.to_csv(removed_columns_file, index=False)\n",
    "    \n",
    "        print(f\"Not Matched columns saved to: {removed_columns_file}\")\n",
    "\n",
    "\n",
    "    # Filter the DataFrame to keep only the selected columns\n",
    "    df_filtered = df_A[cols_to_keep]\n",
    "\n",
    "    # Save the filtered dataset\n",
    "    df_filtered.to_csv(output_file, index=False)\n",
    "    print(f\"Filtered dataset saved to: {output_file}\")\n",
    "    print(f\"List of removed columns saved to: {removed_columns_file}\")\n",
    "\n",
    "    return output_file+\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_below_percentile(file_path_sosec_dataset, percentile=10):\n",
    "    '''\n",
    "    performs filter on i_TIME using the given percentile value\n",
    "    input: csv of sosec dataset.\n",
    "    percentile value\n",
    "    output: filtered dataset as csv\n",
    "    '''\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path_sosec_dataset)\n",
    "\n",
    "    # Calculate the 10th percentile of the 'i_TIME' column\n",
    "    percentile_value = df['i_TIME'].quantile(percentile / 100.0)\n",
    "\n",
    "    # Filter the DataFrame to keep only rows where 'i_TIME' is greater than or equal to the 10th percentile\n",
    "    df_filtered = df[df['i_TIME'] >= percentile_value]\n",
    "\n",
    "    # Save the updated DataFrame back to a CSV file\n",
    "    output_file_path = r\"../data/1_preprocess/7_df_dataset_with_codebook_columns_filtered_itime.csv\"\n",
    "    df_filtered.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Updated dataset saved to: {output_file_path}\")\n",
    "    return output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load CSV and one-hot encode specific columns\n",
    "def one_hot_encode_csv(input_file_path, columns_to_encode):\n",
    "    '''\n",
    "    input: csv of sosec dataset.\n",
    "    list of columns to encode.\n",
    "    deleted the columns_to_encode from the data.\n",
    "    save the csv\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(input_file_path)\n",
    "    \n",
    "    # Select only available columns from the list\n",
    "    available_columns = [col for col in columns_to_encode if col in df.columns]\n",
    "\n",
    "    # One-hot encode the available columns\n",
    "    df_encoded = pd.get_dummies(df, columns=available_columns)\n",
    "\n",
    "    output_file_path = r\"../data/1_preprocess/8_df_dataset_with_codebook_columns_filtered_hotencoding.csv\"\n",
    "\n",
    "    # Save the encoded DataFrame to a new CSV file\n",
    "    df_encoded.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"One-hot encoded CSV saved to {output_file_path}\")\n",
    "    return output_file_path+\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_all_csv_files(folder_path):\n",
    "    \"\"\"\n",
    "    Deletes all CSV files in the specified folder.\n",
    "\n",
    "    Parameters:\n",
    "        folder_path (str): Relative or absolute path to the folder.\n",
    "    \"\"\"\n",
    "    # Get the full path of all CSV files in the folder\n",
    "    csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "    if not (csv_files):\n",
    "        print(f\"No csv files to delete in {folder_path}\")\n",
    "    else:\n",
    "        for file_path in csv_files:\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted: {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete {file_path}: {e}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_personas(FILE_PATH_DATASET,FILE_PATH_REFORMATED_SOSEC_CODE_BOOK):\n",
    "    # Load the files\n",
    "    file1 = pd.read_csv(FILE_PATH_DATASET, sep=\",\")  # Adjust separator if necessary\n",
    "    file2 = pd.read_csv(FILE_PATH_REFORMATED_SOSEC_CODE_BOOK, sep=\",\")  # Adjust separator if necessary\n",
    "\n",
    "    # Create a mapping from file2\n",
    "    mapping = dict(zip(file2['Custom_variable_name'], file2['Text']))\n",
    "\n",
    "    # Rename columns in file1\n",
    "    file1.rename(columns={\"i_TIME\": \"i_TIME (Time taken in seconds to fill the survey)\"}, inplace=True)\n",
    "    file1 = file1.rename(columns=mapping)  # Assign back to file1\n",
    "\n",
    "    # Save the updated file1\n",
    "    output_file_path = r\"../data/1_preprocess/9_processed_data_for_personas.csv\"\n",
    "\n",
    "    file1.to_csv(output_file_path, sep=\",\", index=False)\n",
    "\n",
    "    print(\"Column headers replaced successfully to Questions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: ../data/1_preprocess\\1_codebook_no_matching_columns_in_dataset.csv\n",
      "Deleted: ../data/1_preprocess\\1_df_dataset_with_codebook_columns_full_no_processing.csv\n",
      "Deleted: ../data/1_preprocess\\2_df_dataset_with_codebook_columns_full_F2A3.csv\n",
      "Deleted: ../data/1_preprocess\\3_df_dataset_with_codebook_columns_full_F7mA1.csv\n",
      "Deleted: ../data/1_preprocess\\4_df_dataset_with_codebook_columns_filtered_outofrange.csv\n",
      "Deleted: ../data/1_preprocess\\5_df_dataset_with_codebook_columns_filtered_F7cA1-yob.csv\n",
      "Deleted: ../data/1_preprocess\\6_df_dataset_with_codebook_columns_filtered_lessdata.csv\n",
      "Deleted: ../data/1_preprocess\\6_removed_columns_due_to_lessdata.csv\n",
      "Deleted: ../data/1_preprocess\\7_df_dataset_with_codebook_columns_filtered_itime.csv\n",
      "Deleted: ../data/1_preprocess\\9_processed_data_for_personas.csv\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jim\\AppData\\Local\\Temp\\ipykernel_35920\\368653022.py:8: DtypeWarning: Columns (57,58,62,68,75,76) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path_dataset)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nice! No duplicate column titles found.\n",
      "\n",
      "Updated dataset saved to: ../data/1_preprocess/2_df_dataset_with_codebook_columns_full_F2A3.csv\n",
      "\n",
      "No of rows and columns in original dataset after setting 6,0 to empty in F2A3\n",
      "Number of rows in the csv: 36781\n",
      "Number of columns in the CSV: 96\n",
      "Updated dataset saved to: ../data/1_preprocess/3_df_dataset_with_codebook_columns_full_F7mA1.csv\n",
      "\n",
      "No of rows and columns in original dataset after setting 0 to empty in F7mA1\n",
      "Number of rows in the csv: 36781\n",
      "Number of columns in the CSV: 96\n",
      "Cleaned data saved to '../data/1_preprocess/4_df_dataset_with_codebook_columns_filtered_outofrange.csv'.\n",
      "\n",
      "No of data after removing out of invalid range values from dataset\n",
      "Number of rows in the csv: 36198\n",
      "Number of columns in the CSV: 96\n",
      "Updated dataset saved to: ../data/1_preprocess/5_df_dataset_with_codebook_columns_filtered_F7cA1-yob.csv\n",
      "\n",
      "No of data rows after drop rows having out of range F7cA1 (YOB:1959-2004) values\n",
      "Number of rows in the csv: 22712\n",
      "Number of columns in the CSV: 96\n",
      "Removed columns: ['F5A14_1', 'F5A15_1', 'F5bA2_1', 'F7mA1']\n",
      "Not Matched columns saved to: ../data/1_preprocess/6_removed_columns_due_to_lessdata.csv\n",
      "Filtered dataset saved to: ../data/1_preprocess/6_df_dataset_with_codebook_columns_filtered_lessdata.csv\n",
      "List of removed columns saved to: ../data/1_preprocess/6_removed_columns_due_to_lessdata.csv\n",
      "\n",
      "No of data rows after Delete columns having more than 70% null with exceptions of columns: ['F6a_DemPartyA2', 'F6a_RepPartyA2', 'F6b_DemPartyA2', 'F6b_RepPartyA2'] \n",
      "Number of rows in the csv: 22712\n",
      "Number of columns in the CSV: 92\n",
      "Updated dataset saved to: ../data/1_preprocess/7_df_dataset_with_codebook_columns_filtered_itime.csv\n",
      "\n",
      "No of data rows and col which is filled having time of i_TIME more then 10th percentile\n",
      "Number of rows in the csv: 20455\n",
      "Number of columns in the CSV: 92\n",
      "Column headers replaced successfully to Questions!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#Encode the columns which are based on categorical values. \\n#F7a:Gender\\n#F7bA1\\tEnter a 5-digit Zip number.:\\n#F7d\\tWere you born in the US?\\n#F7e\\tWas your mother born in the US?\\n#F7f\\tWas your father born in the US?\\n#F7g:  educational level\\n#F7h: employment status?\\n#F7i\\tWhat is your marital status?\\n#F7lA1\\tWhich religious community do you belong to?\\n#F7mA1\\tTo which of the following occupational groups do you belong?\\n#F7n\\tWhich ethnic group do you belong to?\\ninput_file_path = output_file_path\\ncolumns_to_encode = [\\'F7a\\', \\'F7bA1\\',\\'F7d\\',\\'F7e\\',\\'F7f\\',\\'F7g\\', \\'F7h\\', \\'F7i\\', \\'F7lA1\\', \\'F7mA1\\',\\'F7n\\']  \\n# Perform one-hot encoding\\noutput_file_path = one_hot_encode_csv(input_file_path, columns_to_encode)\\nprint(\"\\nNo of data rows and columns after encoding and removing the encoded columns.\")\\nno_of_rows = get_row_count(output_file_path)\\nno_of_columns = count_columns_in_csv(output_file_path)\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the required data files.\n",
    "FILE_PATH_DATASET = r\"..\\data\\0_SOSEC Data RCS\\data_sample_35_SOSEC_dataset_us.csv\"\n",
    "FILE_PATH_REFORMATED_SOSEC_CODE_BOOK = r'..\\data\\0_Reformated_SOSEC_Code-book_US_November.csv' \n",
    "VERBOSE = False\n",
    "\n",
    "# Call the function with the relative path to delete all csv files in 1_preprocess folder.\n",
    "delete_all_csv_files(r\"../data/1_preprocess/\")\n",
    "\n",
    "#Get list of columns from formatted sosec code book\n",
    "required_columns, questions_having_subcategories, _i, _j = get_columns_from_formatted_codebook(FILE_PATH_REFORMATED_SOSEC_CODE_BOOK,VERBOSE)\n",
    "\n",
    "# list of column names to keep\n",
    "columns_to_keep =  ['i_TIME'] + required_columns \n",
    "\n",
    "#Only keep the columns as per sosec dataset\n",
    "output_file_path = filter_datasets_for_required_col(FILE_PATH_DATASET,FILE_PATH_REFORMATED_SOSEC_CODE_BOOK,columns_to_keep,VERBOSE)\n",
    "\n",
    "#Check for duplicate columns in the filtered dataset\n",
    "check_duplicate_cols(output_file_path)\n",
    "\n",
    "#replace 6 and 0 in column F2A3 by empty cells\n",
    "output_file_path = perform_filter_on_dataset_F2A3(output_file_path)\n",
    "print(\"\\nNo of rows and columns in original dataset after setting 6,0 to empty in F2A3\")\n",
    "no_of_rows = get_row_count(output_file_path)\n",
    "no_of_columns = count_columns_in_csv(output_file_path)\n",
    "\n",
    "#replace 0 in column F7mA1 (job category) by empty cells\n",
    "output_file_path = perform_filter_on_dataset_F7mA1(output_file_path)\n",
    "print(\"\\nNo of rows and columns in original dataset after setting 0 to empty in F7mA1\")\n",
    "no_of_rows = get_row_count(output_file_path)\n",
    "no_of_columns = count_columns_in_csv(output_file_path)\n",
    "\n",
    "#Remove out of range data\n",
    "output_file_path = delete_rows_for_out_of_range_data(output_file_path,FILE_PATH_REFORMATED_SOSEC_CODE_BOOK,False)\n",
    "print(\"\\nNo of data after removing out of invalid range values from dataset\")\n",
    "no_of_rows = get_row_count(output_file_path)\n",
    "no_of_columns = count_columns_in_csv(output_file_path)\n",
    "\n",
    "\n",
    "#filter rows based on F7cA1 (YOB) values, remove where out of reasonable age range.\n",
    "yob_start = 1959\n",
    "yob_end = 2004\n",
    "output_file_path = perform_filter_on_dataset_F7cA1(output_file_path,1959,2004)\n",
    "print(f\"\\nNo of data rows after drop rows having out of range F7cA1 (YOB:{yob_start}-{yob_end}) values\")\n",
    "no_of_rows = get_row_count(output_file_path)\n",
    "no_of_columns = count_columns_in_csv(output_file_path)\n",
    "\n",
    "#Delete columns having more than 70% null with exceptions of some columns\n",
    "col_to_exclude =[\"F6a_DemPartyA2\",\"F6a_RepPartyA2\",\"F6b_DemPartyA2\",\"F6b_RepPartyA2\"]\n",
    "output_file_path = drop_high_null_columns(output_file_path,FILE_PATH_REFORMATED_SOSEC_CODE_BOOK, col_to_exclude,0.7)\n",
    "print(f\"\\nNo of data rows after Delete columns having more than 70% null with exceptions of columns: {col_to_exclude} \")\n",
    "no_of_rows = get_row_count(output_file_path)\n",
    "no_of_columns = count_columns_in_csv(output_file_path)\n",
    "\n",
    "#Delete data which is filled in rapidly without reading by using i_time column\n",
    "percentile_val = 10\n",
    "output_file_path = remove_below_percentile(output_file_path,10)\n",
    "print(f\"\\nNo of data rows and col which is filled having time of i_TIME more then {percentile_val}th percentile\")\n",
    "no_of_rows = get_row_count(output_file_path)\n",
    "no_of_columns = count_columns_in_csv(output_file_path)\n",
    "\n",
    "#Rename all columns in the data file to questions for creating personas.\n",
    "FILE_PATH_REFORMATED_SOSEC_CODE_BOOK = r'..\\data\\0_Reformated_SOSEC_Code-book_US_November_Reformulated_Questions.csv' \n",
    "prepare_for_personas(output_file_path,FILE_PATH_REFORMATED_SOSEC_CODE_BOOK)\n",
    "\n",
    "\n",
    "#Not applied: hot-encooding.\n",
    "'''\n",
    "#Encode the columns which are based on categorical values. \n",
    "#F7a:Gender\n",
    "#F7bA1\tEnter a 5-digit Zip number.:\n",
    "#F7d\tWere you born in the US?\n",
    "#F7e\tWas your mother born in the US?\n",
    "#F7f\tWas your father born in the US?\n",
    "#F7g:  educational level\n",
    "#F7h: employment status?\n",
    "#F7i\tWhat is your marital status?\n",
    "#F7lA1\tWhich religious community do you belong to?\n",
    "#F7mA1\tTo which of the following occupational groups do you belong?\n",
    "#F7n\tWhich ethnic group do you belong to?\n",
    "input_file_path = output_file_path\n",
    "columns_to_encode = ['F7a', 'F7bA1','F7d','F7e','F7f','F7g', 'F7h', 'F7i', 'F7lA1', 'F7mA1','F7n']  \n",
    "# Perform one-hot encoding\n",
    "output_file_path = one_hot_encode_csv(input_file_path, columns_to_encode)\n",
    "print(\"\\nNo of data rows and columns after encoding and removing the encoded columns.\")\n",
    "no_of_rows = get_row_count(output_file_path)\n",
    "no_of_columns = count_columns_in_csv(output_file_path)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
