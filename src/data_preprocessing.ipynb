{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_specific_files(file_paths):\n",
    "    \"\"\"\n",
    "    Deletes specific files given their paths.\n",
    "\n",
    "    Parameters:\n",
    "        file_paths (list): A list of file paths to delete.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the status of each file.\n",
    "              Format: {file_path: \"Deleted\" or \"Error: <reason>\"}\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            if os.path.exists(file_path):\n",
    "                os.remove(file_path)\n",
    "                result[file_path] = \"Deleted\"\n",
    "            else:\n",
    "                result[file_path] = \"Error: File does not exist\"\n",
    "        except Exception as e:\n",
    "            result[file_path] = f\"Error: {str(e)}\"\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_column_in_csv(input_file, output_file, column_to_drop):\n",
    "    \"\"\"\n",
    "    Drops a specific column from a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        input_file (str): Path to the input CSV file.\n",
    "        output_file (str): Path to the output CSV file with the column removed.\n",
    "        column_to_drop (str): The name of the column to be dropped.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the input CSV\n",
    "        with open(input_file, 'r', newline='', encoding='utf-8') as infile:\n",
    "            reader = csv.DictReader(infile)\n",
    "            fieldnames = [col for col in reader.fieldnames if col != column_to_drop]\n",
    "            \n",
    "            if column_to_drop not in reader.fieldnames:\n",
    "                print(f\"Column '{column_to_drop}' not found in the CSV.\")\n",
    "                return\n",
    "\n",
    "            # Write to the output CSV\n",
    "            with open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "                writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "                for row in reader:\n",
    "                    del row[column_to_drop]\n",
    "                    writer.writerow(row)\n",
    "\n",
    "        print(f\"Column '{column_to_drop}' has been removed and saved to '{output_file}'.\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{input_file}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicate_cols(file_path):\n",
    "    '''\n",
    "    Input: csv file path\n",
    "    Checks if the csv file have duplicate column names.\n",
    "    '''\n",
    "    df = pd.read_csv(file_path,encoding=\"utf-8\")\n",
    "\n",
    "    # Get column names and find duplicates\n",
    "    column_names = df.columns\n",
    "    duplicate_columns = column_names[column_names.duplicated()].tolist()\n",
    "\n",
    "    if duplicate_columns:\n",
    "        print(f\"WARNING! Duplicate column titles found: {duplicate_columns}\")\n",
    "    else:\n",
    "        print(\"\\nNice! No duplicate column titles found.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns_from_formatted_codebook(file_path_formatted_codebook, verbose):\n",
    "    '''\n",
    "    input: formatted codebook filepath\n",
    "    Read the formatted codebook csv to get the questions and their color categories\n",
    "    '''\n",
    "    # Read the dataset from file\n",
    "    df = pd.read_csv(file_path_formatted_codebook)\n",
    "\n",
    "    # Creating no_color_questions for rows where 'color' is empty\n",
    "    no_color_questions = df.loc[df['Color_Category'].isnull(), 'Custom_variable_name'].tolist()\n",
    "\n",
    "    # Creating lists for each color, where grey_questions=question having sub categories.\n",
    "    grey_questions  = df.loc[df['Color_Category'] == 'Grey', 'Custom_variable_name'].tolist()\n",
    "    yellow_questions = df.loc[df['Color_Category'] == 'Yellow', 'Custom_variable_name'].tolist()\n",
    "    green_questions  = df.loc[df['Color_Category'] == 'Green', 'Custom_variable_name'].tolist()\n",
    "    \n",
    "    question_with_characteristics= df.loc[df['Characteristic'] != '-', 'Custom_variable_name'].tolist()\n",
    "\n",
    "    # Output the results if needed\n",
    "    if(verbose):\n",
    "        print(\"::::get_columns_from_formatted_codebook:::::Returns\")\n",
    "        print(\"no_color_questions:\", no_color_questions)\n",
    "        print(\"grey_questions(question having sub categories):\", grey_questions )\n",
    "        print(\"yellow_questions:\", yellow_questions )\n",
    "        print(\"green_questions:\", green_questions )\n",
    "        print(\"green_questions:\", green_questions )\n",
    "    return no_color_questions, grey_questions,yellow_questions, green_questions,question_with_characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_datasets_for_required_col(file_path_dataset,file_path_formatted_sosec_code_book, columns_to_keep, output_folder_path=r\"../data/1_preprocess/\",verbose=False):\n",
    "    '''\n",
    "    input:codebook file path and sosec dataset file path\n",
    "    Only Keeps the columns in the dataset which are also in the codebook and saves the files\n",
    "    also Saves a file for the columns in the code book which are not in the dataset\n",
    "    '''\n",
    "    # Read the dataset CSV file\n",
    "    df = pd.read_csv(file_path_dataset)\n",
    "\n",
    "    # Check which columns are not found\n",
    "    not_found_columns = [col for col in columns_to_keep if col not in df.columns]\n",
    "\n",
    "    if not_found_columns:\n",
    "        # Read the formatted_sosec_code_book CSV file\n",
    "        file_path_2 = file_path_formatted_sosec_code_book\n",
    "        df2 = pd.read_csv(file_path_2,encoding=\"utf-8\")\n",
    "        \n",
    "        # Filter rows where Custom_variable_name matches values in not_found_columns\n",
    "        df_no_matches = df2[df2['Custom_variable_name'].isin(not_found_columns)]\n",
    "        \n",
    "        # Select the corresponding Text values\n",
    "        result_df = df_no_matches[['Custom_variable_name', 'Text']]\n",
    "        \n",
    "        # Save the result as a new CSV\n",
    "        output_file = r'1_codebook_no_matching_columns_in_dataset.csv'\n",
    "        output_file_path = os.path.join(output_folder_path, output_file)\n",
    "        result_df.to_csv(output_file_path, index=False)\n",
    "        if(verbose):\n",
    "            print(f\"Not Matched columns saved to: {output_file_path}\")\n",
    "\n",
    "    # Filter the DataFrame to keep only the columns that exist in the DataFrame\n",
    "    df_filtered = df[[col for col in columns_to_keep if col in df.columns]]\n",
    "    \n",
    "    output_file = r'1_df_dataset_with_codebook_columns_full_no_processing.csv'\n",
    "    output_file_path = os.path.join(output_folder_path, output_file)\n",
    "\n",
    "    df_filtered.to_csv(output_file_path, index=False)\n",
    "\n",
    "    if(verbose):\n",
    "        print(df_filtered.head())\n",
    "    \n",
    "    return output_file_path+\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row_count(file_path_csv):\n",
    "    '''\n",
    "    input filepath of csv\n",
    "    Returns the number of rows in the given CSV file.\n",
    "    '''\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path_csv)\n",
    "\n",
    "    # Get the number of rows\n",
    "    row_count = len(df)\n",
    "\n",
    "    print(f\"Number of rows in the csv: {row_count}\")\n",
    "    return row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_columns_in_csv(file_path_csv):\n",
    "    \"\"\"\n",
    "    Counts and returns the number of columns in a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): The path to the CSV file.\n",
    "    \n",
    "    Returns:\n",
    "    int: The number of columns in the CSV file.\n",
    "    \"\"\"\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path_csv)\n",
    "    \n",
    "    # Get the number of col\n",
    "    columns_count = len(df.columns)\n",
    "    \n",
    "    \n",
    "    print(f\"Number of columns in the CSV: {columns_count}\")\n",
    "    # Return the number of columns\n",
    "    return columns_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Excluded\n",
    "\n",
    "\n",
    "def perform_filter_on_dataset_F2A3(file_path_dataset_with_codebook_columns,output_folder_path=r\"../data/1_preprocess/\"):\n",
    "    '''\n",
    "    input the datafile path with only codebook columns\n",
    "    replace 6 and 0 by blank\n",
    "    F2A3 = The possibility of losing your job (leave this empty if you do not work)\n",
    "    '''\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path_dataset_with_codebook_columns)\n",
    "\n",
    "    # Replace 6 and 0 in the 'F2A3' column with  empty\n",
    "    df['F2A3'] = df['F2A3'].replace({6: \"\", 0: \"\"})\n",
    "\n",
    "    # Save the updated DataFrame back to a CSV file\n",
    "    output_file = r'2_df_dataset_with_codebook_columns_full_F2A3.csv'\n",
    "    output_file_path = os.path.join(output_folder_path, output_file)\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Updated dataset saved to: {output_file_path}\")\n",
    "    return output_file_path+\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Excluded\n",
    "\n",
    "def perform_filter_on_dataset_F7mA1(file_path_dataset_with_codebook_columns,output_folder_path=r\"../data/1_preprocess/\"):\n",
    "    '''\n",
    "    input the datafile path with only codebook columns\n",
    "    replace 0 by blank\n",
    "    F7mA1 = job category (leave this empty if you do not work)\n",
    "    '''\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path_dataset_with_codebook_columns)\n",
    "\n",
    "    # Replace 0 in the 'F7mA1' column with  empty\n",
    "    df['F7mA1'] = df['F7mA1'].replace({0: \"\"})\n",
    "\n",
    "    # Save the updated DataFrame back to a CSV file\n",
    "    output_file = r'3_df_dataset_with_codebook_columns_full_F7mA1.csv'\n",
    "    output_file_path = os.path.join(output_folder_path, output_file)    \n",
    "    df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Updated dataset saved to: {output_file_path}\")\n",
    "    return output_file_path+\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXCLUDED:\n",
    "def delete_rows_for_out_of_range_data(sosec_data_path,file_path_dataset_with_codebook_columns,output_folder_path=r\"../data/1_preprocess/\",verbose=False):\n",
    "    '''\n",
    "    input: sosec_datafile path and sosec dataset with only codebook columns\n",
    "    Deletes the rows for all the columns for with the values are out of range. and saves the csv\n",
    "    '''\n",
    "    file1 = pd.read_csv(sosec_data_path)\n",
    "\n",
    "    file2 = pd.read_csv(file_path_dataset_with_codebook_columns)\n",
    "\n",
    "    # Create a dictionary from file2 with ranges\n",
    "    ranges = {}\n",
    "    for _, row in file2.iterrows():\n",
    "        col = row['Custom_variable_name']\n",
    "        range_str = row['Characteristic']\n",
    "        \n",
    "        # Check if range_str is a valid string before splitting\n",
    "        if isinstance(range_str, str) and range_str and range_str != '-':  # Valid range and not 'F'\n",
    "            # If there's a valid range, split it into a list of integers\n",
    "            ranges[col] = list(map(int, range_str.split(',')))\n",
    "\n",
    "        else:\n",
    "            # If no valid range is provided (empty or 'F'), set the range to None\n",
    "            ranges[col] = None\n",
    "\n",
    "    # Validate the data in file1 against the ranges\n",
    "    def validate_data(file1, ranges):\n",
    "        errors = []\n",
    "        valid_rows = file1.copy()  # Copy of the original DataFrame to modify\n",
    "        \n",
    "        # Loop through each column and validate values\n",
    "        for col in file1.columns:\n",
    "            if col in ranges:\n",
    "                valid_range = ranges[col]\n",
    "                if valid_range is not None:  # Only check if a valid range exists\n",
    "                    # Create a boolean mask for invalid rows\n",
    "                    invalid_rows = ~(valid_rows[col].isin(valid_range) | valid_rows[col].isna())      \n",
    "\n",
    "                    # Track errors for rows with out-of-range data\n",
    "                    for index, value in valid_rows[invalid_rows][col].dropna().items():\n",
    "                        errors.append(f\"Out of range: {col} at row {index + 1} with value {value}\")\n",
    "                    \n",
    "                    # Remove rows with invalid data\n",
    "                    valid_rows = valid_rows[~invalid_rows]\n",
    "                else:\n",
    "                    # If no range is provided, assume all values are valid for that column\n",
    "                    continue\n",
    "        \n",
    "        return valid_rows, errors\n",
    "\n",
    "    # Get valid rows and errors\n",
    "    valid_rows, errors = validate_data(file1, ranges)\n",
    "\n",
    "    # Save the valid rows to a CSV file\n",
    "    output_file = r\"4_df_dataset_with_codebook_columns_filtered_outofrange.csv\"\n",
    "    output_file_path = os.path.join(output_folder_path, output_file)\n",
    "    valid_rows.to_csv(output_file_path, index=False)\n",
    "\n",
    "    if(verbose):\n",
    "        # Output validation errors\n",
    "        if errors:\n",
    "            print(\"Validation Errors:\")\n",
    "            for error in errors:\n",
    "                print(error)\n",
    "        else:\n",
    "            print(\"All data is within valid ranges.\")\n",
    "\n",
    "    # Print where the cleaned data has been saved\n",
    "    print(f\"Cleaned data saved to '{output_file_path}'.\")\n",
    "\n",
    "    return output_file_path+\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#26-01-2025: updated method.\n",
    "def handle_invalid_values(sosec_data_path, file_path_dataset_with_codebook_columns, output_folder_path=r\"../data/1_preprocess/\", verbose=False):\n",
    "    '''\n",
    "    input: sosec_datafile path and sosec dataset with only codebook columns\n",
    "    Replaces out-of-range values with \"\" (null) instead of deleting rows and saves the CSV.\n",
    "    '''\n",
    "    file1 = pd.read_csv(sosec_data_path)\n",
    "    file2 = pd.read_csv(file_path_dataset_with_codebook_columns)\n",
    "\n",
    "    # Create a dictionary from file2 with ranges\n",
    "    ranges = {}\n",
    "    for _, row in file2.iterrows():\n",
    "        col = row['Custom_variable_name']\n",
    "        range_str = row['Characteristic']\n",
    "        \n",
    "        # Check if range_str is a valid string before splitting\n",
    "        if isinstance(range_str, str) and range_str and range_str != '-':  # Valid range and not 'F'\n",
    "            # If there's a valid range, split it into a list of integers\n",
    "            ranges[col] = list(map(int, range_str.split(',')))\n",
    "        else:\n",
    "            # If no valid range is provided (empty or 'F'), set the range to None\n",
    "            ranges[col] = None\n",
    "\n",
    "    # Validate the data in file1 against the ranges\n",
    "    def validate_data(file1, ranges):\n",
    "        errors = []\n",
    "        valid_data = file1.copy()  # Copy of the original DataFrame to modify\n",
    "\n",
    "        # Loop through each column and validate values\n",
    "        for col in file1.columns:\n",
    "            if col in ranges:\n",
    "                valid_range = ranges[col]\n",
    "                if valid_range is not None:  # Only check if a valid range exists\n",
    "                    # Create a boolean mask for invalid values\n",
    "                    invalid_mask = ~(valid_data[col].isin(valid_range) | valid_data[col].isna())\n",
    "\n",
    "                    # Track errors for rows with out-of-range data\n",
    "                    for index, value in valid_data[invalid_mask][col].dropna().items():\n",
    "                        errors.append(f\"Out of range: {col} at row {index + 1} with value {value}\")\n",
    "\n",
    "                    # Replace invalid values with an empty string\n",
    "                    #valid_data.loc[invalid_mask, col] = \"\"\n",
    "                    valid_data.loc[invalid_mask, col] = np.nan  # Assign NaN for invalid values\n",
    "                else:\n",
    "                    # If no range is provided, assume all values are valid for that column\n",
    "                    continue\n",
    "        \n",
    "        return valid_data, errors\n",
    "\n",
    "    # Get valid data and errors\n",
    "    valid_data, errors = validate_data(file1, ranges)\n",
    "\n",
    "    # Save the modified data to a CSV file\n",
    "    output_file = r\"4_df_dataset_with_codebook_columns_filtered_outofrange.csv\"\n",
    "    output_file_path = os.path.join(output_folder_path, output_file)\n",
    "    valid_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "    if verbose:\n",
    "        # Output validation errors\n",
    "        if errors:\n",
    "            print(\"Validation Errors:\")\n",
    "            for error in errors:\n",
    "                print(error)\n",
    "        else:\n",
    "            print(\"All data is within valid ranges.\")\n",
    "\n",
    "    # Print where the cleaned data has been saved\n",
    "    print(f\"Cleaned data saved to '{output_file_path}'.\")\n",
    "\n",
    "    return output_file_path + \"\"\n",
    "\n",
    "# Example usage:\n",
    "# handle_invalid_values(\"sosec_data.csv\", \"codebook_columns.csv\", verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Old method: not in use.\n",
    "\n",
    "def perform_filter_on_dataset_F7cA1(file_path_dataset_with_codebook_columns,min_yob,max_yob,output_folder_path=r\"../data/1_preprocess/\"):\n",
    "    '''\n",
    "    filter rows based on range for F7cA1(Yob).\n",
    "    '''\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path_dataset_with_codebook_columns)\n",
    "\n",
    "    # Filter out rows where 'F7cA1' is not in range\n",
    "    df = df[(df['F7cA1'] >= min_yob) & (df['F7cA1'] <= max_yob)]\n",
    "\n",
    "    # Save the updated DataFrame back to a CSV file\n",
    "    output_file = r'5_df_dataset_with_codebook_columns_filtered_F7cA1-yob.csv'\n",
    "    output_file_path = os.path.join(output_folder_path, output_file)\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Updated dataset saved to: {output_file_path}\")\n",
    "    return output_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODIFIED: 26-01-2025:  invalid values are replaced by null and are not deleted.\n",
    "\n",
    "def perform_filter_on_dataset_F7cA1(file_path_dataset_with_codebook_columns, min_yob, max_yob, output_folder_path=r\"../data/1_preprocess/\"):\n",
    "    '''\n",
    "    Replace out-of-range values with \"\" (null) for F7cA1 (Year of Birth).\n",
    "    '''\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path_dataset_with_codebook_columns)\n",
    "\n",
    "    # Replace values in 'F7cA1' that are out of range with \"\" (null)\n",
    "    out_of_range_mask = (df['F7cA1'] < min_yob) | (df['F7cA1'] > max_yob)\n",
    "    #df.loc[out_of_range_mask, 'F7cA1'] = \"\"\n",
    "    df.loc[out_of_range_mask, 'F7cA1'] = np.nan  # Assign NaN for invalid values\n",
    "\n",
    "    # Save the updated DataFrame back to a CSV file\n",
    "    output_file = r'5_df_dataset_with_codebook_columns_filtered_F7cA1-yob.csv'\n",
    "    output_file_path = os.path.join(output_folder_path, output_file)\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Updated dataset saved to: {output_file_path}\")\n",
    "    return output_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_codefile_YOB_char_and_label(file_path, custom_variable_name, yob_start, yob_end, output_file):\n",
    "    \"\"\"\n",
    "    Updates the Characteristic and Value_labels in the CSV file for a specific Custom_variable_name\n",
    "    and saves the updated data to a new file.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "        custom_variable_name (str): The Custom_variable_name to search for.\n",
    "        yob_start (int): Start year for Characteristic.\n",
    "        yob_end (int): End year for Characteristic.\n",
    "        output_file (str): Path to save the updated CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    updated_rows = []\n",
    "    updated_characteristic = \", \".join(str(year) for year in range(yob_start, yob_end + 1))\n",
    "    updated_value_labels = f\"{yob_start} to {yob_end}\"\n",
    "\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        with open(file_path, 'r', newline='', encoding='utf-8') as infile:\n",
    "            reader = csv.DictReader(infile)\n",
    "            fieldnames = reader.fieldnames\n",
    "            \n",
    "            # Ensure the required columns exist\n",
    "            if \"Custom_variable_name\" not in fieldnames or \"Characteristic\" not in fieldnames or \"Value_labels\" not in fieldnames:\n",
    "                print(\"Error: Required columns (Custom_variable_name, Characteristic, Value_labels) are missing in the CSV.\")\n",
    "                return\n",
    "\n",
    "            # Process each row\n",
    "            for row in reader:\n",
    "                if row[\"Custom_variable_name\"] == custom_variable_name:\n",
    "                    row[\"Characteristic\"] = updated_characteristic\n",
    "                    row[\"Value_labels\"] = updated_value_labels\n",
    "                updated_rows.append(row)\n",
    "\n",
    "        # Write to the new output CSV file\n",
    "        with open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(updated_rows)\n",
    "\n",
    "        print(f\"Updated CSV file has been saved as '{output_file}'.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_high_null_columns(FILE_PATH_DATASET,file_path_formatted_sosec_code_book ,col_to_exclude, threshold=0.7 ,output_folder_path=r\"../data/1_preprocess/\"):\n",
    "    '''\n",
    "    Deletes columns with more than 70% null values from file_name_A, excluding from list of columns and saves the dataset.\n",
    "    Prints removed columns and saves them to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        file_name_A (str): Path to the input CSV file.\n",
    "        col_to_exclude(list): Name of the column to exclude from deletion.\n",
    "        threshold (float): Proportion of nulls above which columns are dropped.\n",
    "        output_file (str): Path to save the filtered dataset.\n",
    "        removed_columns_file (str): Path to save the list of removed columns.\n",
    "    '''\n",
    "\n",
    "    output_file_csv = r\"6_df_dataset_with_codebook_columns_filtered_lessdata.csv\"\n",
    "    output_file = os.path.join(output_folder_path, output_file_csv)\n",
    "    # Load the dataset\n",
    "    df_A = pd.read_csv(FILE_PATH_DATASET)\n",
    "    \n",
    "    # Calculate the threshold for null values\n",
    "    null_threshold = threshold * len(df_A)\n",
    "    \n",
    "    # Identify columns to keep based on null percentage and exceptions\n",
    "    cols_to_keep = [col for col in df_A.columns \n",
    "                    if (df_A[col].isna().sum() <= null_threshold) or col in col_to_exclude]\n",
    "    \n",
    "    # Identify the columns to remove\n",
    "    cols_to_remove = [col for col in df_A.columns if col not in cols_to_keep]\n",
    "    \n",
    "    # Print the removed columns\n",
    "    print(f\"Removed columns: {cols_to_remove}\")\n",
    "    \n",
    "\n",
    "    if cols_to_remove:\n",
    "        # Read the formatted_sosec_code_book CSV file\n",
    "        file_path_2 = file_path_formatted_sosec_code_book\n",
    "        df2 = pd.read_csv(file_path_2,encoding=\"utf-8\")\n",
    "        \n",
    "        # Filter rows where Custom_variable_name matches values in not_found_columns\n",
    "        df_no_matches = df2[df2['Custom_variable_name'].isin(cols_to_remove)]\n",
    "        \n",
    "        # Select the corresponding Text values\n",
    "        result_df = df_no_matches[['Custom_variable_name', 'Text']]\n",
    "        \n",
    "        # Save the result as a new CSV\n",
    "        output_file_csv= r\"6_columns_having_70percent_empty_values.csv\"\n",
    "        removed_columns_file = os.path.join(output_folder_path, output_file_csv)\n",
    "        result_df.to_csv(removed_columns_file, index=False)\n",
    "    \n",
    "        print(f\"Not Matched columns saved to: {removed_columns_file}\")\n",
    "\n",
    "\n",
    "    # Filter the DataFrame to keep only the selected columns\n",
    "    df_filtered = df_A[cols_to_keep]\n",
    "\n",
    "    # Save the filtered dataset\n",
    "    df_filtered.to_csv(output_file, index=False)\n",
    "    print(f\"Filtered dataset saved to: {output_file}\")\n",
    "    print(f\"List of removed columns saved to: {removed_columns_file}\")\n",
    "    \n",
    "    #Note: filter not performed. \n",
    "    return FILE_PATH_DATASET+\"\" #output_file+\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_below_percentile(file_path_sosec_dataset, percentile=10,output_folder_path=r\"../data/1_preprocess/\"):\n",
    "    '''\n",
    "    performs filter on i_TIME using the given percentile value\n",
    "    input: csv of sosec dataset.\n",
    "    percentile value\n",
    "    output: filtered dataset as csv\n",
    "    '''\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path_sosec_dataset)\n",
    "\n",
    "    # Calculate the 10th percentile of the 'i_TIME' column\n",
    "    percentile_value = df['i_TIME'].quantile(percentile / 100.0)\n",
    "\n",
    "    # Filter the DataFrame to keep only rows where 'i_TIME' is greater than or equal to the 10th percentile\n",
    "    df_filtered = df[df['i_TIME'] >= percentile_value]\n",
    "\n",
    "    # Save the updated DataFrame back to a CSV file\n",
    "    output_file = r\"7_df_dataset_with_codebook_columns_filtered_itime.csv\"\n",
    "    output_file_path = os.path.join(output_folder_path, output_file)\n",
    "    df_filtered.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Updated dataset saved to: {output_file_path}\")\n",
    "    return output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load CSV and one-hot encode specific columns\n",
    "def one_hot_encode_csv(df, columns_to_encode,output_folder_path=r\"../data/1_combined_preprocess/\"):\n",
    "    '''\n",
    "    input: csv of sosec dataset.\n",
    "    list of columns to encode.\n",
    "    deleted the columns_to_encode from the data.\n",
    "    save the csv\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # Select only available columns from the list\n",
    "    available_columns = [col for col in columns_to_encode if col in df.columns]\n",
    "\n",
    "    # One-hot encode the available columns\n",
    "    df_encoded = pd.get_dummies(df, columns=available_columns)\n",
    "\n",
    "    output_file = r\"8_df_dataset_with_codebook_columns_filtered_hotencoding.csv\"\n",
    "    output_file_path = os.path.join(output_folder_path, output_file)\n",
    "    # Save the encoded DataFrame to a new CSV file\n",
    "    df_encoded.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"One-hot encoded CSV saved to {output_file_path}\")\n",
    "    return output_file_path+\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_all_csv_files(folder_path):\n",
    "    \"\"\"\n",
    "    Deletes all CSV files in the specified folder.\n",
    "\n",
    "    Parameters:\n",
    "        folder_path (str): Relative or absolute path to the folder.\n",
    "    \"\"\"\n",
    "    # Get the full path of all CSV files in the folder\n",
    "    csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "    if not (csv_files):\n",
    "        print(f\"No csv files to delete in {folder_path}\")\n",
    "    else:\n",
    "        for file_path in csv_files:\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted: {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete {file_path}: {e}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_personas_format1(FILE_PATH_DATASET,FILE_PATH_REFORMATED_SOSEC_CODE_BOOK,output_folder_path=r\"../data/1_preprocess/\"):\n",
    "    # Load the files\n",
    "    file1_path = FILE_PATH_REFORMATED_SOSEC_CODE_BOOK\n",
    "    file2_path = FILE_PATH_DATASET\n",
    "\n",
    "    file1 = pd.read_csv(file1_path)\n",
    "    file2 = pd.read_csv(file2_path)\n",
    "\n",
    "    # Get the column names from file2 (excluding i_TIME)\n",
    "    file2.rename(columns={\"i_TIME\": \"i_TIME (Time taken in seconds to fill the survey)\"}, inplace=True)\n",
    "    file2_columns = file2.columns[0:]\n",
    "\n",
    "    # Check which columns in file2 exist in the Custom_variable_name of file1\n",
    "    common_columns = file1[file1['Custom_variable_name'].isin(file2_columns)]\n",
    "\n",
    "    # Extract the required values\n",
    "    text_row = common_columns.set_index('Custom_variable_name').reindex(file2_columns)['Text'].fillna('').values.tolist()\n",
    "    characteristic_row = common_columns.set_index('Custom_variable_name').reindex(file2_columns)['Characteristic'].fillna('').values.tolist()\n",
    "    value_labels_row = common_columns.set_index('Custom_variable_name').reindex(file2_columns)['Value_labels'].fillna('').values.tolist()\n",
    "\n",
    "    # Insert these rows into file2\n",
    "    file2 = pd.DataFrame(file2) \n",
    "    new_rows = pd.DataFrame([text_row, characteristic_row, value_labels_row], columns=file2.columns)\n",
    "\n",
    "    # Combine the new rows with the original data\n",
    "    file2 = pd.concat([new_rows, file2], ignore_index=True)\n",
    "\n",
    "    # Save the updated file2 to a new CSV\n",
    "    output_file = r\"9_processed_data_for_personas_Format_1.csv\"\n",
    "    FILE_PATH_PERSONA_FORMAT1 = os.path.join(output_folder_path, output_file)\n",
    "    file2.to_csv(FILE_PATH_PERSONA_FORMAT1, index=False)\n",
    "\n",
    "    print(f\"Updated file2 saved to {FILE_PATH_PERSONA_FORMAT1}\")\n",
    "    return FILE_PATH_PERSONA_FORMAT1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_personas_format2(FILE_PATH_PERSONA_FORMAT1,output_folder_path=r\"../data/1_preprocess/\"):\n",
    "    # Load the files\n",
    "    df= pd.read_csv(FILE_PATH_PERSONA_FORMAT1, header=None) \n",
    "\n",
    "    # Save the updated file1\n",
    "    output_file = r\"9_processed_data_for_personas_Format_2.csv\"\n",
    "    output_file_path = os.path.join(output_folder_path, output_file)\n",
    "\n",
    "    # Remove the first row (the old header row)\n",
    "    df = df.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "    # Save the updated DataFrame back to a CSV file without the header\n",
    "    df.to_csv(output_file_path, header=False, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates_and_matches(file1, file2, output_duplicates_file1='duplicates_file1.csv',\n",
    "                                 output_duplicates_file2='duplicates_file2.csv', output_matches='matching_rows.csv'):\n",
    "    \"\"\"\n",
    "    Check for duplicates in two CSV files and identify matching rows between them.\n",
    "\n",
    "    Parameters:\n",
    "    - file1: str - Path to the first CSV file.\n",
    "    - file2: str - Path to the second CSV file.\n",
    "    - output_duplicates_file1: str - Path to save duplicate rows from file1 (default: 'duplicates_file1.csv').\n",
    "    - output_duplicates_file2: str - Path to save duplicate rows from file2 (default: 'duplicates_file2.csv').\n",
    "    - output_matches: str - Path to save matching rows between file1 and file2 (default: 'matching_rows.csv').\n",
    "\n",
    "    Returns:\n",
    "    - None: Saves duplicates and matching rows to the specified files.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the first file\n",
    "    print(\"\\nLoading file1...\")\n",
    "    df1 = pd.read_csv(file1)\n",
    "    print(f\"File1 loaded: {len(df1)} rows.\")\n",
    "\n",
    "    # Load the second file\n",
    "    print(\"\\nLoading file2...\")\n",
    "    df2 = pd.read_csv(file2)\n",
    "    print(f\"File2 loaded: {len(df2)} rows.\")\n",
    "\n",
    "    # Step 1: Check for duplicates in file1\n",
    "    print(\"\\n\\nChecking for duplicates in file1...\")\n",
    "    duplicates_file1 = df1[df1.duplicated(keep=False)]\n",
    "    if not duplicates_file1.empty:\n",
    "        print(f\"Found {len(duplicates_file1)} duplicate rows in file1.\")\n",
    "        duplicates_file1.to_csv(output_duplicates_file1, index=False)\n",
    "        print(f\"Duplicate rows saved to {output_duplicates_file1}.\")\n",
    "    else:\n",
    "        print(\"No duplicate rows found in file1.\")\n",
    "\n",
    "    # Step 2: Check for duplicates in file2\n",
    "    print(\"\\n\\nChecking for duplicates in file2...\")\n",
    "    duplicates_file2 = df2[df2.duplicated(keep=False)]\n",
    "    if not duplicates_file2.empty:\n",
    "        print(f\"Found {len(duplicates_file2)} duplicate rows in file2.\")\n",
    "        duplicates_file2.to_csv(output_duplicates_file2, index=False)\n",
    "        print(f\"Duplicate rows saved to {output_duplicates_file2}.\")\n",
    "    else:\n",
    "        print(\"No duplicate rows found in file2.\")\n",
    "\n",
    "    # Step 3: Check for matching rows between file1 and file2\n",
    "    print(\"\\n\\nChecking for matching rows between file1 and file2...\")\n",
    "    df1_set = set(tuple(row) for row in df1.values)\n",
    "    df2_set = set(tuple(row) for row in df2.values)\n",
    "\n",
    "    # Find intersections (matching rows)\n",
    "    matching_rows = df1_set.intersection(df2_set)\n",
    "\n",
    "    if matching_rows:\n",
    "        print(f\"Found {len(matching_rows)} matching row(s) between file1 and file2.\")\n",
    "        matching_rows_df = pd.DataFrame(list(matching_rows), columns=df1.columns)\n",
    "        matching_rows_df.to_csv(output_matches, index=False)\n",
    "        print(f\"Matching rows saved to {output_matches}.\")\n",
    "    else:\n",
    "        print(\"No matching rows found between file1 and file2.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading file1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jim\\AppData\\Local\\Temp\\ipykernel_10072\\1290577118.py:19: DtypeWarning: Columns (57,58,62,68,75,76) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df1 = pd.read_csv(file1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File1 loaded: 36781 rows.\n",
      "\n",
      "Loading file2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jim\\AppData\\Local\\Temp\\ipykernel_10072\\1290577118.py:24: DtypeWarning: Columns (57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,75,76) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df2 = pd.read_csv(file2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File2 loaded: 33513 rows.\n",
      "\n",
      "\n",
      "Checking for duplicates in file1...\n",
      "No duplicate rows found in file1.\n",
      "\n",
      "\n",
      "Checking for duplicates in file2...\n",
      "No duplicate rows found in file2.\n",
      "\n",
      "\n",
      "Checking for matching rows between file1 and file2...\n",
      "No matching rows found between file1 and file2.\n"
     ]
    }
   ],
   "source": [
    "file1 = r\"..\\data\\0_SOSEC Data RCS\\data_sample_35_SOSEC_dataset_us.csv\"\n",
    "file2 = r\"..\\data\\0_SOSEC Data RCS\\new_data_sample.csv\"\n",
    "\n",
    "check_duplicates_and_matches(\n",
    "    file1=file1,\n",
    "    file2=file2,\n",
    "    output_duplicates_file1=r\"..\\data\\0_preprocess\\duplicates_file1.csv\",\n",
    "    output_duplicates_file2=r\"..\\data\\0_preprocess\\duplicates_file2.csv\",\n",
    "    output_matches=r\"..\\data\\0_preprocess\\matching_rows.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_files(df1, df2, output_file, remove_duplicates=True):\n",
    "    \"\"\"\n",
    "    Merge rows from file2 into file1 and save as a new CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - df1: str - Path to the first CSV df.\n",
    "    - df2: str - Path to the second CSV df.\n",
    "    - output_file: str - Path to save the merged file.\n",
    "    - remove_duplicates: bool - Whether to remove duplicate rows after merging (default: True).\n",
    "\n",
    "    Returns:\n",
    "    - None: Saves the merged rows to the specified file.\n",
    "    \"\"\"\n",
    "    # Merge rows\n",
    "    print(\"Merging df...\")\n",
    "    merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "    print(f\"Files merged: {len(merged_df)} rows before removing duplicates.\")\n",
    "\n",
    "    # Remove duplicates if required\n",
    "    if remove_duplicates:\n",
    "        merged_df = merged_df.drop_duplicates()\n",
    "        print(f\"Duplicates removed: {len(merged_df)} rows after removing duplicates.\")\n",
    "\n",
    "    # Save to output file\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "    print(f\"Merged file saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jim\\AppData\\Local\\Temp\\ipykernel_10072\\3969281207.py:10: DtypeWarning: Columns (57,58,62,68,75,76) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df1 = pd.read_csv(file1)\n",
      "C:\\Users\\Jim\\AppData\\Local\\Temp\\ipykernel_10072\\3969281207.py:11: DtypeWarning: Columns (57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,75,76) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df2 = pd.read_csv(file2, skiprows=[0])  # Skip rows 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging df...\n",
      "Files merged: 70293 rows before removing duplicates.\n",
      "Merged file saved to ..\\data\\0_SOSEC Data RCS\\0_combined_data.csv.\n"
     ]
    }
   ],
   "source": [
    "file1 =  r\"..\\data\\0_SOSEC Data RCS\\data_sample_35_SOSEC_dataset_us.csv\"\n",
    "file2  = r\"..\\data\\0_SOSEC Data RCS\\new_data_sample.csv\"\n",
    "\n",
    "\n",
    "output_file = r\"..\\data\\0_SOSEC Data RCS\\0_combined_data.csv\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(file1)  \n",
    "df2 = pd.read_csv(file2, skiprows=[0])  # Skip rows 0\n",
    "\n",
    "\n",
    "merge_files(df1, df2, output_file,remove_duplicates=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIRST SAMPLE FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: ../data/1a_preprocess\\1_codebook_no_matching_columns_in_dataset.csv\n",
      "Deleted: ../data/1a_preprocess\\9_processed_data_for_personas_Format_1.csv\n",
      "Deleted: ../data/1a_preprocess\\9_processed_data_for_personas_Format_2.csv\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jim\\AppData\\Local\\Temp\\ipykernel_10072\\1971867800.py:8: DtypeWarning: Columns (57,58,62,68,75,76) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path_dataset)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nice! No duplicate column titles found.\n",
      "\n",
      "Cleaned data saved to '../data/1a_preprocess/4_df_dataset_with_codebook_columns_filtered_outofrange.csv'.\n",
      "\n",
      "No of data after HANDLING out of invalid range values from dataset: NOTE SHOULD BE SAME\n",
      "Number of rows in the csv: 36781\n",
      "Number of columns in the CSV: 133\n",
      "Updated dataset saved to: ../data/1a_preprocess/5_df_dataset_with_codebook_columns_filtered_F7cA1-yob.csv\n",
      "\n",
      "No of data rows after FILTER rows having out of range F7cA1 (YOB:1959-2004) values: NOTE SHOULD BE SAME\n",
      "Number of rows in the csv: 36781\n",
      "Number of columns in the CSV: 133\n",
      "Updated CSV file has been saved as '../data/SOSEC_Code-book_Current.csv'.\n",
      "Updated dataset saved to: ../data/1a_preprocess/7_df_dataset_with_codebook_columns_filtered_itime.csv\n",
      "\n",
      "No of data rows and col which is filled having time of i_TIME more then 10th percentile\n",
      "Number of rows in the csv: 33118\n",
      "Number of columns in the CSV: 133\n",
      "Column 'i_TIME' has been removed and saved to '../data/1a_preprocess/7a_df_dataset_with_codebook_columns_filtered_itime.csv'.\n",
      "Updated file2 saved to ../data/1a_preprocess/9_processed_data_for_personas_Format_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jim\\AppData\\Local\\Temp\\ipykernel_10072\\704620123.py:3: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df= pd.read_csv(FILE_PATH_PERSONA_FORMAT1, header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/1a_preprocess/1_df_dataset_with_codebook_columns_full_no_processing.csv: Deleted\n",
      "../data/1a_preprocess/2_df_dataset_with_codebook_columns_full_F2A3.csv: Error: File does not exist\n",
      "../data/1a_preprocess/3_df_dataset_with_codebook_columns_full_F7mA1.csv: Error: File does not exist\n",
      "../data/1a_preprocess/4_df_dataset_with_codebook_columns_filtered_outofrange.csv: Deleted\n",
      "../data/1a_preprocess/5_df_dataset_with_codebook_columns_filtered_F7cA1-yob.csv: Deleted\n",
      "../data/1a_preprocess/6_df_dataset_with_codebook_columns_filtered_lessdata.csv: Error: File does not exist\n",
      "../data/1a_preprocess/7_df_dataset_with_codebook_columns_filtered_itime.csv: Deleted\n",
      "../data/1a_preprocess/7a_df_dataset_with_codebook_columns_filtered_itime.csv: Deleted\n"
     ]
    }
   ],
   "source": [
    "# Load the required data files.\n",
    "FILE_PATH_DATASET = r\"..\\data\\0_SOSEC Data RCS\\data_sample_35_SOSEC_dataset_us.csv\"\n",
    "\n",
    "#FILE_PATH_DATASET = r\"..\\data\\0_SOSEC Data RCS\\0_combined_data.csv\"\n",
    "\n",
    "#FILE_PATH_DATASET = r\"..\\data\\0_SOSEC Data RCS\\new_data_sample.csv\"\n",
    "\n",
    "\n",
    "\n",
    "FILE_PATH_REFORMATED_SOSEC_CODE_BOOK = r'..\\data\\0_Reformated_SOSEC_Code-book_US_November_Reformulated_Questions_For_Dict_All_Columns.csv' \n",
    "OUTPUT_FOLDER_PATH=r\"../data/1a_preprocess/\"\n",
    "\n",
    "VERBOSE = False\n",
    "\n",
    "# Call the function with the relative path to delete all csv files in 1_preprocess folder.\n",
    "delete_all_csv_files(r\"../data/1a_preprocess/\")\n",
    "\n",
    "#Get list of columns from formatted sosec code book\n",
    "#no_color_questions, grey_questions,yellow_questions, green_questions,question_with_characteristics\n",
    "_i, _j, _k, _l,required_columns = get_columns_from_formatted_codebook(FILE_PATH_REFORMATED_SOSEC_CODE_BOOK,VERBOSE)\n",
    "\n",
    "# list of column names to keep\n",
    "columns_to_keep =  ['i_TIME'] + required_columns \n",
    "\n",
    "#Only keep the columns as per sosec dataset\n",
    "output_file_path = filter_datasets_for_required_col(FILE_PATH_DATASET,FILE_PATH_REFORMATED_SOSEC_CODE_BOOK,columns_to_keep,OUTPUT_FOLDER_PATH,VERBOSE)\n",
    "\n",
    "#Check for duplicate columns in the filtered dataset\n",
    "check_duplicate_cols(output_file_path)\n",
    "\n",
    "#Changed the 0 to 6 in the codebook.\n",
    "#replace 6 and 0 in column F2A3 by empty cells\n",
    "#output_file_path = perform_filter_on_dataset_F2A3(output_file_path,OUTPUT_FOLDER_PATH)\n",
    "#print(\"\\nNo of rows and columns in original dataset after setting 6,0 to empty in F2A3\")\n",
    "#no_of_rows = get_row_count(output_file_path)\n",
    "#no_of_columns = count_columns_in_csv(output_file_path)\n",
    "\n",
    "#replace 0 in column F7mA1 (job category) by empty cells\n",
    "#output_file_path = perform_filter_on_dataset_F7mA1(output_file_path,OUTPUT_FOLDER_PATH)\n",
    "#print(\"\\nNo of rows and columns in original dataset after setting 0 to empty in F7mA1\")\n",
    "#no_of_rows = get_row_count(output_file_path)\n",
    "#no_of_columns = count_columns_in_csv(output_file_path)\n",
    "\n",
    "#MODIFIED:(01-26-2025: change out of range data to null instead of deleting/removing the rows)\n",
    "#output_file_path = delete_rows_for_out_of_range_data(output_file_path,FILE_PATH_REFORMATED_SOSEC_CODE_BOOK,OUTPUT_FOLDER_PATH,False)\n",
    "#print(\"\\nNo of data after removing out of invalid range values from dataset\")\n",
    "#no_of_rows = get_row_count(output_file_path)\n",
    "#no_of_columns = count_columns_in_csv(output_file_path)\n",
    "output_file_path = handle_invalid_values(output_file_path,FILE_PATH_REFORMATED_SOSEC_CODE_BOOK,OUTPUT_FOLDER_PATH,False)\n",
    "print(\"\\nNo of data after HANDLING out of invalid range values from dataset: NOTE SHOULD BE SAME\")\n",
    "no_of_rows = get_row_count(output_file_path)\n",
    "no_of_columns = count_columns_in_csv(output_file_path)\n",
    "\n",
    "#MODIFIED:(01-26-2025: change out of range data to null instead of deleting/removing the rows)\n",
    "#filter rows based on F7cA1 (YOB) values, 01-26-2025:DOES NOT remove where out of reasonable age range.\n",
    "yob_start = 1959\n",
    "yob_end = 2004\n",
    "output_file_path = perform_filter_on_dataset_F7cA1(output_file_path,1959,2004,OUTPUT_FOLDER_PATH)\n",
    "print(f\"\\nNo of data rows after FILTER rows having out of range F7cA1 (YOB:{yob_start}-{yob_end}) values: NOTE SHOULD BE SAME\")\n",
    "no_of_rows = get_row_count(output_file_path)\n",
    "no_of_columns = count_columns_in_csv(output_file_path)\n",
    "\n",
    "\n",
    "\n",
    "#Correct YOB range in code book for later use as temp file\n",
    "file_path = r\"../data/0_Reformated_SOSEC_Code-book_US_November_Reformulated_Questions_For_Dict_All_Columns.csv\"\n",
    "SOSEC_Codebook_path = r\"../data/SOSEC_Code-book_Current.csv\"\n",
    "custom_variable_name = \"F7cA1\"\n",
    "yob_start = 1959\n",
    "yob_end = 2004\n",
    "update_codefile_YOB_char_and_label(file_path, custom_variable_name, yob_start, yob_end, SOSEC_Codebook_path)\n",
    "\n",
    "\n",
    "FILE_PATH_REFORMATED_SOSEC_CODE_BOOK = SOSEC_Codebook_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#27-01-2025: removal of columns with null>70%  not applied. \n",
    "#Delete columns having more than 70% null with exceptions of some columns\n",
    "#col_to_exclude =[\"F6a_DemPartyA2\",\"F6a_RepPartyA2\",\"F6b_DemPartyA2\",\"F6b_RepPartyA2\"]\n",
    "#output_file_path = drop_high_null_columns(output_file_path,FILE_PATH_REFORMATED_SOSEC_CODE_BOOK, col_to_exclude,0.7,OUTPUT_FOLDER_PATH)\n",
    "#print(f\"\\nNo of data rows after Delete columns having more than 70% null with exceptions of columns: {col_to_exclude} \")\n",
    "#print(\"NOTE: 70% NULL NOT REMOVED FROM THE FILE\")\n",
    "#no_of_rows = get_row_count(output_file_path)\n",
    "#no_of_columns = count_columns_in_csv(output_file_path)\n",
    "\n",
    "#Delete data which is filled in rapidly without reading by using i_time column\n",
    "percentile_val = 10\n",
    "output_file_path = remove_below_percentile(output_file_path,10,OUTPUT_FOLDER_PATH)\n",
    "print(f\"\\nNo of data rows and col which is filled having time of i_TIME more then {percentile_val}th percentile\")\n",
    "no_of_rows = get_row_count(output_file_path)\n",
    "no_of_columns = count_columns_in_csv(output_file_path)\n",
    "\n",
    "#Drop i_time\n",
    "input_csv=output_file_path\n",
    "output_file_path=r\"../data/1a_preprocess/7a_df_dataset_with_codebook_columns_filtered_itime.csv\"\n",
    "column_to_remove='i_TIME'\n",
    "drop_column_in_csv(input_csv, output_file_path, column_to_remove)\n",
    "\n",
    "\n",
    "#Add info as rows for creating personas.\n",
    "FILE_PATH_PERSONA_FORMAT1= prepare_for_personas_format1(output_file_path,FILE_PATH_REFORMATED_SOSEC_CODE_BOOK,OUTPUT_FOLDER_PATH)\n",
    "\n",
    "\n",
    "#Rename all first row of row labels.\n",
    "prepare_for_personas_format2(FILE_PATH_PERSONA_FORMAT1,OUTPUT_FOLDER_PATH)\n",
    "\n",
    "\n",
    "#Delete generated extra files.\n",
    "file_paths = [\n",
    "    #r\"../data/1a_preprocess/1_codebook_no_matching_columns_in_dataset.csv\",\n",
    "    r\"../data/1a_preprocess/1_df_dataset_with_codebook_columns_full_no_processing.csv\",\n",
    "    r\"../data/1a_preprocess/2_df_dataset_with_codebook_columns_full_F2A3.csv\",\n",
    "    r\"../data/1a_preprocess/3_df_dataset_with_codebook_columns_full_F7mA1.csv\",\n",
    "    r\"../data/1a_preprocess/4_df_dataset_with_codebook_columns_filtered_outofrange.csv\",\n",
    "    r\"../data/1a_preprocess/5_df_dataset_with_codebook_columns_filtered_F7cA1-yob.csv\",\n",
    "    r\"../data/1a_preprocess/6_df_dataset_with_codebook_columns_filtered_lessdata.csv\",\n",
    "    #r\"../data/1a_preprocess/6_removed_columns_due_to_lessdata.csv\",\n",
    "    r\"../data/1a_preprocess/7_df_dataset_with_codebook_columns_filtered_itime.csv\",\n",
    "    r\"../data/1a_preprocess/7a_df_dataset_with_codebook_columns_filtered_itime.csv\"]\n",
    "\n",
    "status = delete_specific_files(file_paths)\n",
    "for file, message in status.items():\n",
    "    print(f\"{file}: {message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEW SAMPLE FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: ../data/1b_preprocess\\1_codebook_no_matching_columns_in_dataset.csv\n",
      "Deleted: ../data/1b_preprocess\\9_processed_data_for_personas_Format_1.csv\n",
      "Deleted: ../data/1b_preprocess\\9_processed_data_for_personas_Format_2.csv\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jim\\AppData\\Local\\Temp\\ipykernel_10072\\1971867800.py:8: DtypeWarning: Columns (57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,75,76) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path_dataset)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nice! No duplicate column titles found.\n",
      "\n",
      "Cleaned data saved to '../data/1b_preprocess/4_df_dataset_with_codebook_columns_filtered_outofrange.csv'.\n",
      "\n",
      "No of data after HANDLING out of invalid range values from dataset: NOTE SHOULD BE SAME\n",
      "Number of rows in the csv: 33513\n",
      "Number of columns in the CSV: 133\n",
      "Updated dataset saved to: ../data/1b_preprocess/5_df_dataset_with_codebook_columns_filtered_F7cA1-yob.csv\n",
      "\n",
      "No of data rows after FILTER rows having out of range F7cA1 (YOB:1959-2004) values: NOTE SHOULD BE SAME\n",
      "Number of rows in the csv: 33513\n",
      "Number of columns in the CSV: 133\n",
      "Updated CSV file has been saved as '../data/SOSEC_Code-book_Current.csv'.\n",
      "Updated dataset saved to: ../data/1b_preprocess/7_df_dataset_with_codebook_columns_filtered_itime.csv\n",
      "\n",
      "No of data rows and col which is filled having time of i_TIME more then 10th percentile\n",
      "Number of rows in the csv: 30168\n",
      "Number of columns in the CSV: 133\n",
      "Column 'i_TIME' has been removed and saved to '../data/1b_preprocess/7a_df_dataset_with_codebook_columns_filtered_itime.csv'.\n",
      "Updated file2 saved to ../data/1b_preprocess/9_processed_data_for_personas_Format_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jim\\AppData\\Local\\Temp\\ipykernel_10072\\704620123.py:3: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df= pd.read_csv(FILE_PATH_PERSONA_FORMAT1, header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/1b_preprocess/1_df_dataset_with_codebook_columns_full_no_processing.csv: Deleted\n",
      "../data/1b_preprocess/2_df_dataset_with_codebook_columns_full_F2A3.csv: Error: File does not exist\n",
      "../data/1b_preprocess/3_df_dataset_with_codebook_columns_full_F7mA1.csv: Error: File does not exist\n",
      "../data/1b_preprocess/4_df_dataset_with_codebook_columns_filtered_outofrange.csv: Deleted\n",
      "../data/1b_preprocess/5_df_dataset_with_codebook_columns_filtered_F7cA1-yob.csv: Deleted\n",
      "../data/1b_preprocess/6_df_dataset_with_codebook_columns_filtered_lessdata.csv: Error: File does not exist\n",
      "../data/1b_preprocess/7_df_dataset_with_codebook_columns_filtered_itime.csv: Deleted\n",
      "../data/1b_preprocess/7a_df_dataset_with_codebook_columns_filtered_itime.csv: Deleted\n"
     ]
    }
   ],
   "source": [
    "# Load the required data files.\n",
    "FILE_PATH_DATASET = r\"..\\data\\0_SOSEC Data RCS\\new_data_sample.csv\"\n",
    "FILE_PATH_REFORMATED_SOSEC_CODE_BOOK = r'..\\data\\0_Reformated_SOSEC_Code-book_US_November_Reformulated_Questions_For_Dict_All_Columns.csv' \n",
    "OUTPUT_FOLDER_PATH=r\"../data/1b_preprocess/\"\n",
    "\n",
    "VERBOSE = False\n",
    "\n",
    "# Call the function with the relative path to delete all csv files in 1_preprocess folder.\n",
    "delete_all_csv_files(r\"../data/1b_preprocess/\")\n",
    "\n",
    "#Get list of columns from formatted sosec code book\n",
    "#no_color_questions, grey_questions,yellow_questions, green_questions,question_with_characteristics\n",
    "_i, _j, _k, _l,required_columns = get_columns_from_formatted_codebook(FILE_PATH_REFORMATED_SOSEC_CODE_BOOK,VERBOSE)\n",
    "\n",
    "# list of column names to keep\n",
    "columns_to_keep =  ['i_TIME'] + required_columns \n",
    "\n",
    "#Only keep the columns as per sosec dataset\n",
    "output_file_path = filter_datasets_for_required_col(FILE_PATH_DATASET,FILE_PATH_REFORMATED_SOSEC_CODE_BOOK,columns_to_keep,OUTPUT_FOLDER_PATH,VERBOSE)\n",
    "\n",
    "#Check for duplicate columns in the filtered dataset\n",
    "check_duplicate_cols(output_file_path)\n",
    "\n",
    "#Changed the 0 to 6 in the codebook.\n",
    "#replace 6 and 0 in column F2A3 by empty cells\n",
    "#output_file_path = perform_filter_on_dataset_F2A3(output_file_path,OUTPUT_FOLDER_PATH)\n",
    "#print(\"\\nNo of rows and columns in original dataset after setting 6,0 to empty in F2A3\")\n",
    "#no_of_rows = get_row_count(output_file_path)\n",
    "#no_of_columns = count_columns_in_csv(output_file_path)\n",
    "\n",
    "#replace 0 in column F7mA1 (job category) by empty cells\n",
    "#output_file_path = perform_filter_on_dataset_F7mA1(output_file_path,OUTPUT_FOLDER_PATH)\n",
    "#print(\"\\nNo of rows and columns in original dataset after setting 0 to empty in F7mA1\")\n",
    "#no_of_rows = get_row_count(output_file_path)\n",
    "#no_of_columns = count_columns_in_csv(output_file_path)\n",
    "\n",
    "#MODIFIED:(01-26-2025: change out of range data to null instead of deleting/removing the rows)\n",
    "#output_file_path = delete_rows_for_out_of_range_data(output_file_path,FILE_PATH_REFORMATED_SOSEC_CODE_BOOK,OUTPUT_FOLDER_PATH,False)\n",
    "#print(\"\\nNo of data after removing out of invalid range values from dataset\")\n",
    "#no_of_rows = get_row_count(output_file_path)\n",
    "#no_of_columns = count_columns_in_csv(output_file_path)\n",
    "output_file_path = handle_invalid_values(output_file_path,FILE_PATH_REFORMATED_SOSEC_CODE_BOOK,OUTPUT_FOLDER_PATH,False)\n",
    "print(\"\\nNo of data after HANDLING out of invalid range values from dataset: NOTE SHOULD BE SAME\")\n",
    "no_of_rows = get_row_count(output_file_path)\n",
    "no_of_columns = count_columns_in_csv(output_file_path)\n",
    "\n",
    "#MODIFIED:(01-26-2025: change out of range data to null instead of deleting/removing the rows)\n",
    "#filter rows based on F7cA1 (YOB) values, 01-26-2025:DOES NOT remove where out of reasonable age range.\n",
    "yob_start = 1959\n",
    "yob_end = 2004\n",
    "output_file_path = perform_filter_on_dataset_F7cA1(output_file_path,1959,2004,OUTPUT_FOLDER_PATH)\n",
    "print(f\"\\nNo of data rows after FILTER rows having out of range F7cA1 (YOB:{yob_start}-{yob_end}) values: NOTE SHOULD BE SAME\")\n",
    "no_of_rows = get_row_count(output_file_path)\n",
    "no_of_columns = count_columns_in_csv(output_file_path)\n",
    "\n",
    "#Correct YOB range in code book for later use as temp file\n",
    "file_path = r\"../data/0_Reformated_SOSEC_Code-book_US_November_Reformulated_Questions_For_Dict_All_Columns.csv\"\n",
    "SOSEC_Codebook_path = r\"../data/SOSEC_Code-book_Current.csv\"\n",
    "custom_variable_name = \"F7cA1\"\n",
    "yob_start = 1959\n",
    "yob_end = 2004\n",
    "update_codefile_YOB_char_and_label(file_path, custom_variable_name, yob_start, yob_end, SOSEC_Codebook_path)\n",
    "\n",
    "FILE_PATH_REFORMATED_SOSEC_CODE_BOOK = SOSEC_Codebook_path\n",
    "\n",
    "#27-01-2025: removal of columns with null>70%  not applied. \n",
    "#Delete columns having more than 70% null with exceptions of some columns\n",
    "#col_to_exclude =[\"F6a_DemPartyA2\",\"F6a_RepPartyA2\",\"F6b_DemPartyA2\",\"F6b_RepPartyA2\"]\n",
    "#output_file_path = drop_high_null_columns(output_file_path,FILE_PATH_REFORMATED_SOSEC_CODE_BOOK, col_to_exclude,0.7,OUTPUT_FOLDER_PATH)\n",
    "#print(f\"\\nNo of data rows after Delete columns having more than 70% null with exceptions of columns: {col_to_exclude} \")\n",
    "#print(\"NOTE: 70% NULL NOT REMOVED FROM THE FILE\")\n",
    "#no_of_rows = get_row_count(output_file_path)\n",
    "#no_of_columns = count_columns_in_csv(output_file_path)\n",
    "\n",
    "#Delete data which is filled in rapidly without reading by using i_time column\n",
    "percentile_val = 10\n",
    "output_file_path = remove_below_percentile(output_file_path,10,OUTPUT_FOLDER_PATH)\n",
    "print(f\"\\nNo of data rows and col which is filled having time of i_TIME more then {percentile_val}th percentile\")\n",
    "no_of_rows = get_row_count(output_file_path)\n",
    "no_of_columns = count_columns_in_csv(output_file_path)\n",
    "\n",
    "#Drop i_time\n",
    "input_csv=output_file_path\n",
    "output_file_path=r\"../data/1b_preprocess/7a_df_dataset_with_codebook_columns_filtered_itime.csv\"\n",
    "column_to_remove='i_TIME'\n",
    "drop_column_in_csv(input_csv, output_file_path, column_to_remove)\n",
    "\n",
    "#Add info as rows for creating personas.\n",
    "FILE_PATH_PERSONA_FORMAT1= prepare_for_personas_format1(output_file_path,FILE_PATH_REFORMATED_SOSEC_CODE_BOOK,OUTPUT_FOLDER_PATH)\n",
    "\n",
    "#Rename all first row of row labels.\n",
    "prepare_for_personas_format2(FILE_PATH_PERSONA_FORMAT1,OUTPUT_FOLDER_PATH)\n",
    "\n",
    "#Delete generated extra files.\n",
    "file_paths = [\n",
    "    #r\"../data/1a_preprocess/1_codebook_no_matching_columns_in_dataset.csv\",\n",
    "    r\"../data/1b_preprocess/1_df_dataset_with_codebook_columns_full_no_processing.csv\",\n",
    "    r\"../data/1b_preprocess/2_df_dataset_with_codebook_columns_full_F2A3.csv\",\n",
    "    r\"../data/1b_preprocess/3_df_dataset_with_codebook_columns_full_F7mA1.csv\",\n",
    "    r\"../data/1b_preprocess/4_df_dataset_with_codebook_columns_filtered_outofrange.csv\",\n",
    "    r\"../data/1b_preprocess/5_df_dataset_with_codebook_columns_filtered_F7cA1-yob.csv\",\n",
    "    r\"../data/1b_preprocess/6_df_dataset_with_codebook_columns_filtered_lessdata.csv\",\n",
    "    #r\"../data/1b_preprocess/6_removed_columns_due_to_lessdata.csv\",\n",
    "    r\"../data/1b_preprocess/7_df_dataset_with_codebook_columns_filtered_itime.csv\",\n",
    "    r\"../data/1b_preprocess/7a_df_dataset_with_codebook_columns_filtered_itime.csv\"\n",
    "    ]\n",
    "\n",
    "status = delete_specific_files(file_paths)\n",
    "for file, message in status.items():\n",
    "    print(f\"{file}: {message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MERGE RESULTS (FORMAT-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jim\\AppData\\Local\\Temp\\ipykernel_10072\\3047393312.py:6: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df1 = pd.read_csv(file1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging df...\n",
      "Files merged: 63289 rows before removing duplicates.\n",
      "Merged file saved to ..\\data\\1_combined_preprocess\\9_processed_data_for_personas_Format_1.csv.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/1b_preprocess/7a_df_dataset_with_codebook_columns_filtered_itime.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m df2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file2, skiprows\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m])  \u001b[38;5;66;03m# Skip rows 0, 1, 2\u001b[39;00m\n\u001b[0;32m      9\u001b[0m merge_files(df1, df2, output_file,remove_duplicates\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 11\u001b[0m no_of_rows \u001b[38;5;241m=\u001b[39m \u001b[43mget_row_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m no_of_columns \u001b[38;5;241m=\u001b[39m count_columns_in_csv(output_file_path)\n",
      "Cell \u001b[1;32mIn[46], line 7\u001b[0m, in \u001b[0;36mget_row_count\u001b[1;34m(file_path_csv)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03minput filepath of csv\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mReturns the number of rows in the given CSV file.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path_csv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Get the number of rows\u001b[39;00m\n\u001b[0;32m     10\u001b[0m row_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df)\n",
      "File \u001b[1;32mc:\\Users\\Jim\\Desktop\\Desk_10\\RCS\\Research_Case_Agent_Modeling\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jim\\Desktop\\Desk_10\\RCS\\Research_Case_Agent_Modeling\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Jim\\Desktop\\Desk_10\\RCS\\Research_Case_Agent_Modeling\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jim\\Desktop\\Desk_10\\RCS\\Research_Case_Agent_Modeling\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Jim\\Desktop\\Desk_10\\RCS\\Research_Case_Agent_Modeling\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/1b_preprocess/7a_df_dataset_with_codebook_columns_filtered_itime.csv'"
     ]
    }
   ],
   "source": [
    "file1 = r\"..\\data\\1a_preprocess\\9_processed_data_for_personas_Format_1.csv\"\n",
    "file2 = r\"..\\data\\1b_preprocess\\9_processed_data_for_personas_Format_1.csv\"\n",
    "output_file = r\"..\\data\\1_combined_preprocess\\9_processed_data_for_personas_Format_1.csv\"\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(file1)  \n",
    "df2 = pd.read_csv(file2, skiprows=[1, 2, 3])  # Skip rows 0, 1, 2\n",
    "\n",
    "merge_files(df1, df2, output_file,remove_duplicates=False)\n",
    "\n",
    "no_of_rows = get_row_count(output_file_path)\n",
    "no_of_columns = count_columns_in_csv(output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hot encoding on Format #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoded CSV saved to ../data/1_combined_preprocess/8_df_dataset_with_codebook_columns_filtered_hotencoding.csv\n",
      "\n",
      "No of data rows and columns after encoding and removing the encoded columns.\n",
      "Number of rows in the csv: 63286\n",
      "Number of columns in the CSV: 959\n"
     ]
    }
   ],
   "source": [
    "#Not applied: hot-encooding.\n",
    "# Enabled : again on 02-03-25 with all columns.\n",
    "\n",
    "\n",
    "#Encode the columns which are based on categorical values. \n",
    "#F7a:Gender\n",
    "#F7bA1\tEnter a 5-digit Zip number.:\n",
    "#F7d\tWere you born in the US?\n",
    "#F7e\tWas your mother born in the US?\n",
    "#F7f\tWas your father born in the US?\n",
    "#F7g:  educational level\n",
    "#F7h: employment status?\n",
    "#F7i\tWhat is your marital status?\n",
    "#F7lA1\tWhich religious community do you belong to?\n",
    "#F7mA1\tTo which of the following occupational groups do you belong?\n",
    "#F7n\tWhich ethnic group do you belong to?\n",
    "input_file_path = output_file\n",
    "columns_to_encode = ['F7a', 'F7bA1','F7d','F7e','F7f','F7g', 'F7h', 'F7i', 'F7lA1', 'F7mA1','F7n']  \n",
    "dfx = pd.read_csv(input_file_path, skiprows=[1, 2, 3])  # Skip rows 1, 2, 3\n",
    "columns_to_encode = dfx.columns.tolist()  \n",
    "output_folder_path=r\"../data/1_combined_preprocess/\"\n",
    "\n",
    "# Perform one-hot encoding\n",
    "output_file_path = one_hot_encode_csv(dfx, columns_to_encode,output_folder_path)\n",
    "print(\"\\nNo of data rows and columns after encoding and removing the encoded columns.\")\n",
    "no_of_rows = get_row_count(output_file_path)\n",
    "no_of_columns = count_columns_in_csv(output_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MERGE RESULTS (FORMAT-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jim\\AppData\\Local\\Temp\\ipykernel_10072\\1029523502.py:5: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df1 = pd.read_csv(file1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging df...\n",
      "Files merged: 63287 rows before removing duplicates.\n",
      "Merged file saved to ..\\data\\1_combined_preprocess\\9_processed_data_for_personas_Format_2.csv.\n",
      "Number of rows in the csv: 63286\n",
      "Number of columns in the CSV: 959\n"
     ]
    }
   ],
   "source": [
    "file1 = r\"..\\data\\1a_preprocess\\9_processed_data_for_personas_Format_2.csv\"\n",
    "file2 = r\"..\\data\\1b_preprocess\\9_processed_data_for_personas_Format_2.csv\"\n",
    "output_file = r\"..\\data\\1_combined_preprocess\\9_processed_data_for_personas_Format_2.csv\"\n",
    "\n",
    "df1 = pd.read_csv(file1)  \n",
    "df2 = pd.read_csv(file2, skiprows=[0, 1, 2])  # Skip rows 0, 1, 2\n",
    "\n",
    "\n",
    "merge_files(df1, df2, output_file,remove_duplicates=False)\n",
    "\n",
    "no_of_rows = get_row_count(output_file_path)\n",
    "no_of_columns = count_columns_in_csv(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
