{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicate_cols(file_path):\n",
    "    '''\n",
    "    Input: csv file path\n",
    "    Checks if the csv file have duplicate column names.\n",
    "    '''\n",
    "    df = pd.read_csv(file_path,encoding=\"utf-8\")\n",
    "\n",
    "    # Get column names and find duplicates\n",
    "    column_names = df.columns\n",
    "    duplicate_columns = column_names[column_names.duplicated()].tolist()\n",
    "\n",
    "    if duplicate_columns:\n",
    "        print(f\"WARNING! Duplicate column titles found: {duplicate_columns}\")\n",
    "    else:\n",
    "        print(\"Nice! No duplicate column titles found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns_from_formatted_codebook(file_path_formatted_codebook, verbose):\n",
    "    '''\n",
    "    input: formated codebook filepath\n",
    "    Read the formatted codebook csv to get the questions and their color categories\n",
    "    '''\n",
    "    # Read the dataset from file\n",
    "    df = pd.read_csv(file_path_formatted_codebook)\n",
    "\n",
    "    # Creating no_color_questions for rows where 'color' is empty\n",
    "    no_color_questions = df.loc[df['Color_Category'].isnull(), 'Custom_variable_name'].tolist()\n",
    "\n",
    "    # Creating lists for each color, where grey_questions=question having sub categories.\n",
    "    grey_questions  = df.loc[df['Color_Category'] == 'Grey', 'Custom_variable_name'].tolist()\n",
    "    yellow_questions = df.loc[df['Color_Category'] == 'Yellow', 'Custom_variable_name'].tolist()\n",
    "    green_questions  = df.loc[df['Color_Category'] == 'Green', 'Custom_variable_name'].tolist()\n",
    "\n",
    "    # Output the results if needed\n",
    "    if(verbose):\n",
    "        print(\"::::get_columns_from_formatted_codebook:::::Returns\")\n",
    "        print(\"no_color_questions:\", no_color_questions)\n",
    "        print(\"grey_questions(question having sub categories):\", grey_questions )\n",
    "        print(\"yellow_questions:\", yellow_questions )\n",
    "        print(\"green_questions:\", green_questions )\n",
    "    return no_color_questions, grey_questions,yellow_questions, green_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_datasets_for_required_col(file_path_dataset,file_path_formatted_sosec_code_book, columns_to_keep, verbose=False):\n",
    "    '''\n",
    "    input:codebook file path and sosec dataset file path\n",
    "    Only Keeps the columns in the dataset which are also in the codebook and saves the files\n",
    "    also Saves a file for the columns in the code book which are not in the dataset\n",
    "    '''\n",
    "    # Read the dataset CSV file\n",
    "    df = pd.read_csv(file_path_dataset)\n",
    "\n",
    "    # Check which columns are not found\n",
    "    not_found_columns = [col for col in columns_to_keep if col not in df.columns]\n",
    "\n",
    "    if not_found_columns:\n",
    "        # Read the formatted_sosec_code_book CSV file\n",
    "        file_path_2 = file_path_formatted_sosec_code_book\n",
    "        df2 = pd.read_csv(file_path_2,encoding=\"utf-8\")\n",
    "        \n",
    "        # Filter rows where Custom_variable_name matches values in not_found_columns\n",
    "        df_no_matches = df2[df2['Custom_variable_name'].isin(not_found_columns)]\n",
    "        \n",
    "        # Select the corresponding Text values\n",
    "        result_df = df_no_matches[['Custom_variable_name', 'Text']]\n",
    "        \n",
    "        # Save the result as a new CSV\n",
    "        output_file_path = r'../data/1_codebook_no_matching_columns_in_dataset.csv'\n",
    "        result_df.to_csv(output_file_path, index=False)\n",
    "        if(verbose):\n",
    "            print(f\"Not Matched columns saved to: {output_file_path}\")\n",
    "\n",
    "    # Filter the DataFrame to keep only the columns that exist in the DataFrame\n",
    "    df_filtered = df[[col for col in columns_to_keep if col in df.columns]]\n",
    "    \n",
    "    output_file_path = r'../data/1_df_dataset_with_codebook_columns_full_no_processing.csv'\n",
    "    df_filtered.to_csv(output_file_path, index=False)\n",
    "\n",
    "    if(verbose):\n",
    "        print(df_filtered.head())\n",
    "    \n",
    "    return output_file_path+\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row_count(file_path_csv):\n",
    "    '''\n",
    "    input filepath of csv\n",
    "    Returns the number of rows in the given CSV file.\n",
    "    '''\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path_csv)\n",
    "\n",
    "    # Get the number of rows\n",
    "    row_count = len(df)\n",
    "\n",
    "    print(f\"Number of rows in the csv: {row_count}\")\n",
    "    return row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_columns_in_csv(file_path_csv):\n",
    "    \"\"\"\n",
    "    Counts and returns the number of columns in a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): The path to the CSV file.\n",
    "    \n",
    "    Returns:\n",
    "    int: The number of columns in the CSV file.\n",
    "    \"\"\"\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path_csv)\n",
    "    \n",
    "    # Get the number of col\n",
    "    columns_count = len(df.columns)\n",
    "    \n",
    "    \n",
    "    print(f\"Number of columns in the CSV: {columns_count}\")\n",
    "    # Return the number of columns\n",
    "    return columns_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_filter_on_dataset_F2A3(file_path_dataset_with_codebook_columns):\n",
    "    '''\n",
    "    input the datafile path with only codebook columns\n",
    "    replace 6 and 0 by blank\n",
    "    F2A3 = The possibility of losing your job (leave this empty if you do not work)\n",
    "    '''\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path_dataset_with_codebook_columns)\n",
    "\n",
    "    # Replace 6 and 0 in the 'F2A3' column with  empty\n",
    "    df['F2A3'] = df['F2A3'].replace({6: \"\", 0: \"\"})\n",
    "\n",
    "    # Save the updated DataFrame back to a CSV file\n",
    "    output_file_path = r'../data/2_df_dataset_with_codebook_columns_full_F2A3.csv'\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Updated dataset saved to: {output_file_path}\")\n",
    "    return output_file_path+\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_filter_on_dataset_F7mA1(file_path_dataset_with_codebook_columns):\n",
    "    '''\n",
    "    input the datafile path with only codebook columns\n",
    "    replace 0 by blank\n",
    "    F7mA1 = job category (leave this empty if you do not work)\n",
    "    '''\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path_dataset_with_codebook_columns)\n",
    "\n",
    "    # Replace 0 in the 'F7mA1' column with  empty\n",
    "    df['F7mA1'] = df['F7mA1'].replace({0: \"\"})\n",
    "\n",
    "    # Save the updated DataFrame back to a CSV file\n",
    "    output_file_path = r'../data/3_df_dataset_with_codebook_columns_full_F7mA1.csv'\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Updated dataset saved to: {output_file_path}\")\n",
    "    return output_file_path+\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_rows_for_out_of_range_data(sosec_data_path,file_path_dataset_with_codebook_columns,verbose=False):\n",
    "    '''\n",
    "    input: sosec_datafile path and sosec dataset with only codebook columns\n",
    "    Deletes the rows for all the columns for with the values are out of range. and saves the csv\n",
    "    '''\n",
    "    file1 = pd.read_csv(sosec_data_path)\n",
    "\n",
    "    file2 = pd.read_csv(file_path_dataset_with_codebook_columns)\n",
    "\n",
    "    # Create a dictionary from file2 with ranges\n",
    "    ranges = {}\n",
    "    for _, row in file2.iterrows():\n",
    "        col = row['Custom_variable_name']\n",
    "        range_str = row['Characteristic']\n",
    "        \n",
    "        # Check if range_str is a valid string before splitting\n",
    "        if isinstance(range_str, str) and range_str and range_str != 'F':  # Valid range and not 'F'\n",
    "            # If there's a valid range, split it into a list of integers\n",
    "            ranges[col] = list(map(int, range_str.split(',')))\n",
    "        else:\n",
    "            # If no valid range is provided (empty or 'F'), set the range to None\n",
    "            ranges[col] = None\n",
    "\n",
    "\n",
    "    # Validate the data in file1 against the ranges\n",
    "    def validate_data(file1, ranges):\n",
    "        errors = []\n",
    "        valid_rows = file1.copy()  # Copy of the original DataFrame to modify\n",
    "        \n",
    "        # Loop through each column and validate values\n",
    "        for col in file1.columns:\n",
    "            if col in ranges:\n",
    "                valid_range = ranges[col]\n",
    "                if valid_range is not None:  # Only check if a valid range exists\n",
    "                    # Create a boolean mask for invalid rows\n",
    "                    invalid_rows = ~(valid_rows[col].isin(valid_range) | valid_rows[col].isna())      \n",
    "\n",
    "                    # Track errors for rows with out-of-range data\n",
    "                    for index, value in valid_rows[invalid_rows][col].dropna().items():\n",
    "                        errors.append(f\"Out of range: {col} at row {index + 1} with value {value}\")\n",
    "                    \n",
    "                    # Remove rows with invalid data\n",
    "                    valid_rows = valid_rows[~invalid_rows]\n",
    "                else:\n",
    "                    # If no range is provided, assume all values are valid for that column\n",
    "                    continue\n",
    "        \n",
    "        return valid_rows, errors\n",
    "\n",
    "    # Get valid rows and errors\n",
    "    valid_rows, errors = validate_data(file1, ranges)\n",
    "\n",
    "    # Save the valid rows to a CSV file\n",
    "    output_file_path = r\"../data/4_df_dataset_with_codebook_columns_filtered_outofrange.csv\"\n",
    "    valid_rows.to_csv(output_file_path, index=False)\n",
    "\n",
    "    if(verbose):\n",
    "        # Output validation errors\n",
    "        if errors:\n",
    "            print(\"Validation Errors:\")\n",
    "            for error in errors:\n",
    "                print(error)\n",
    "        else:\n",
    "            print(\"All data is within valid ranges.\")\n",
    "\n",
    "    # Print where the cleaned data has been saved\n",
    "    print(f\"Cleaned data saved to '{output_file_path}'.\")\n",
    "\n",
    "    return output_file_path+\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_filter_on_dataset_F7cA1(file_path_dataset_with_codebook_columns,min_yob,max_yob):\n",
    "    '''\n",
    "    filter rows based on range for F7cA1(Yob).\n",
    "    '''\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path_dataset_with_codebook_columns)\n",
    "\n",
    "    # Filter out rows where 'F7cA1' is not in range\n",
    "    df = df[(df['F7cA1'] >= min_yob) & (df['F7cA1'] <= max_yob)]\n",
    "\n",
    "    # Save the updated DataFrame back to a CSV file\n",
    "    output_file_path = r'../data/5_df_dataset_with_codebook_columns_filtered_F7cA1-yob.csv'\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Updated dataset saved to: {output_file_path}\")\n",
    "    return output_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not implemented. May be removed at later stage if not required. \n",
    "def clean_and_filter_dataset_F7bA1(input_dateset_file_path, zipcode_file):\n",
    "    '''\n",
    "    input: csv sosec dataset with only required columns, zipcode_usa csv\n",
    "    Cleans the dataset by:\n",
    "    1. Dropping rows with any empty or NaN values.\n",
    "    2. Keeping rows where F7bA1 matches a ZipCode in US_zipcodes.csv.\n",
    "    '''\n",
    "    # Load the datasets\n",
    "    df = pd.read_csv(input_dateset_file_path)\n",
    "    zipcodes_df = pd.read_csv(zipcode_file)\n",
    "\n",
    "    # Drop rows with any empty or NaN values\n",
    "    df = df.dropna(subset=['F7bA1'])\n",
    "\n",
    "    # Filter rows where F7bA1 values are in the ZipCode column of US_zipcodes\n",
    "    valid_zipcodes = set(zipcodes_df['ZipCodes']) \n",
    "    df = df[df['F7bA1'].isin(valid_zipcodes)]\n",
    "\n",
    "    # Save the cleaned and filtered DataFrame back to a CSV file\n",
    "    output_file_path = r'../data/6_df_dataset_with_codebook_columns_filtered_zip.csv'\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Cleaned and filtered dataset saved to: {output_file_path}\")\n",
    "    return output_file_path+\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_high_null_columns(FILE_PATH_DATASET,file_path_formatted_sosec_code_book ,col_to_exclude, threshold=0.7 ):\n",
    "    '''\n",
    "    Deletes columns with more than 70% null values from file_name_A, excluding from list of columns and saves the dataset.\n",
    "    Prints removed columns and saves them to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        file_name_A (str): Path to the input CSV file.\n",
    "        col_to_exclude(list): Name of the column to exclude from deletion.\n",
    "        threshold (float): Proportion of nulls above which columns are dropped.\n",
    "        output_file (str): Path to save the filtered dataset.\n",
    "        removed_columns_file (str): Path to save the list of removed columns.\n",
    "    '''\n",
    "\n",
    "    output_file = r\"../data/7_df_dataset_with_codebook_columns_filtered_lessdata.csv\"\n",
    "\n",
    "    # Load the dataset\n",
    "    df_A = pd.read_csv(FILE_PATH_DATASET)\n",
    "    \n",
    "    # Calculate the threshold for null values\n",
    "    null_threshold = threshold * len(df_A)\n",
    "    \n",
    "    # Identify columns to keep based on null percentage and exceptions\n",
    "    cols_to_keep = [col for col in df_A.columns \n",
    "                    if (df_A[col].isna().sum() <= null_threshold) or col in col_to_exclude]\n",
    "    \n",
    "    # Identify the columns to remove\n",
    "    cols_to_remove = [col for col in df_A.columns if col not in cols_to_keep]\n",
    "    \n",
    "    # Print the removed columns\n",
    "    print(f\"Removed columns: {cols_to_remove}\")\n",
    "    \n",
    "\n",
    "    if cols_to_remove:\n",
    "        # Read the formatted_sosec_code_book CSV file\n",
    "        file_path_2 = file_path_formatted_sosec_code_book\n",
    "        df2 = pd.read_csv(file_path_2,encoding=\"utf-8\")\n",
    "        \n",
    "        # Filter rows where Custom_variable_name matches values in not_found_columns\n",
    "        df_no_matches = df2[df2['Custom_variable_name'].isin(cols_to_remove)]\n",
    "        \n",
    "        # Select the corresponding Text values\n",
    "        result_df = df_no_matches[['Custom_variable_name', 'Text']]\n",
    "        \n",
    "        # Save the result as a new CSV\n",
    "        removed_columns_file= r\"../data/7_removed_columns_due_to_lessdata.csv\"\n",
    "        result_df.to_csv(removed_columns_file, index=False)\n",
    "    \n",
    "        print(f\"Not Matched columns saved to: {removed_columns_file}\")\n",
    "\n",
    "\n",
    "    # Filter the DataFrame to keep only the selected columns\n",
    "    df_filtered = df_A[cols_to_keep]\n",
    "\n",
    "    # Save the filtered dataset\n",
    "    df_filtered.to_csv(output_file, index=False)\n",
    "    print(f\"Filtered dataset saved to: {output_file}\")\n",
    "    print(f\"List of removed columns saved to: {removed_columns_file}\")\n",
    "\n",
    "    return output_file+\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_below_percentile(file_path_sosec_dataset, percentile=10):\n",
    "    '''\n",
    "    performs filter on i_TIME using the given percentile value\n",
    "    input: csv of sosec dataset.\n",
    "    percentile value\n",
    "    output: filtered dataset as csv\n",
    "    '''\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path_sosec_dataset)\n",
    "\n",
    "    # Calculate the 10th percentile of the 'i_TIME' column\n",
    "    percentile_value = df['i_TIME'].quantile(percentile / 100.0)\n",
    "\n",
    "    # Filter the DataFrame to keep only rows where 'i_TIME' is greater than or equal to the 10th percentile\n",
    "    df_filtered = df[df['i_TIME'] >= percentile_value]\n",
    "\n",
    "    # Save the updated DataFrame back to a CSV file\n",
    "    output_file_path = r\"../data/8_df_dataset_with_codebook_columns_filtered_itime.csv\"\n",
    "    df_filtered.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Updated dataset saved to: {output_file_path}\")\n",
    "    return output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load CSV and one-hot encode specific columns\n",
    "def one_hot_encode_csv(input_file_path, columns_to_encode):\n",
    "    '''\n",
    "    input: csv of sosec dataset.\n",
    "    list of columns to encode.\n",
    "    deleted the columns_to_encode from the data.\n",
    "    save the csv\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(input_file_path)\n",
    "    \n",
    "    # Select only available columns from the list\n",
    "    available_columns = [col for col in columns_to_encode if col in df.columns]\n",
    "\n",
    "    # One-hot encode the available columns\n",
    "    df_encoded = pd.get_dummies(df, columns=available_columns)\n",
    "\n",
    "    output_file_path = r\"../data/9_df_dataset_with_codebook_columns_filtered_hotencoding.csv\"\n",
    "\n",
    "    # Save the encoded DataFrame to a new CSV file\n",
    "    df_encoded.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"One-hot encoded CSV saved to {output_file_path}\")\n",
    "    return output_file_path+\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jim\\AppData\\Local\\Temp\\ipykernel_21584\\4286196345.py:8: DtypeWarning: Columns (57,58,62,68,75,76) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path_dataset)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice! No duplicate column titles found.\n",
      "Updated dataset saved to: ../data/2_df_dataset_with_codebook_columns_full_F2A3.csv\n",
      "\n",
      " No of data in original dataset\n",
      "Number of rows in the csv: 36781\n",
      "Number of columns in the CSV: 96\n",
      "Updated dataset saved to: ../data/3_df_dataset_with_codebook_columns_full_F7mA1.csv\n",
      "\n",
      " No of data in original dataset\n",
      "Number of rows in the csv: 36781\n",
      "Number of columns in the CSV: 96\n",
      "Cleaned data saved to '../data/4_df_dataset_with_codebook_columns_filtered_outofrange.csv'.\n",
      "\n",
      " No of data after removing out of invalid range values from dataset\n",
      "Number of rows in the csv: 35087\n",
      "Number of columns in the CSV: 96\n",
      "Updated dataset saved to: ../data/5_df_dataset_with_codebook_columns_filtered_F7cA1-yob.csv\n",
      "\n",
      " No of data rows after drop rows having out of range F7cA1 (YOB) values\n",
      "Number of rows in the csv: 22075\n",
      "Number of columns in the CSV: 96\n",
      "\n",
      "\n",
      "Removed columns: ['F5A14_1', 'F5A15_1', 'F5bA2_1', 'F7mA1']\n",
      "Not Matched columns saved to: ../data/7_removed_columns_due_to_lessdata.csv\n",
      "Filtered dataset saved to: ../data/7_df_dataset_with_codebook_columns_filtered_lessdata.csv\n",
      "List of removed columns saved to: ../data/7_removed_columns_due_to_lessdata.csv\n",
      "\n",
      " No of data rows after Delete columns having more than 70% null with exceptions of some columns\n",
      "Number of rows in the csv: 22075\n",
      "Number of columns in the CSV: 92\n",
      "Updated dataset saved to: ../data/8_df_dataset_with_codebook_columns_filtered_itime.csv\n",
      "\n",
      " No of data rows which is filled in rapidly without reading using time of i_TIME less then some percentile value\n",
      "Number of rows in the csv: 19875\n",
      "Number of columns in the CSV: 92\n"
     ]
    }
   ],
   "source": [
    "# Load the required data files.\n",
    "FILE_PATH_DATASET = r\"..\\data\\SOSEC Data RCS\\data_sample_35_SOSEC_dataset_us.csv\"\n",
    "FILE_PATH_REFORMATED_SOSEC_CODE_BOOK = r'..\\data\\0_Reformated_SOSEC_Code-book_US_November.csv' \n",
    "VERBOSE = False\n",
    "\n",
    "#Get list of columns from formatted sosec code book\n",
    "required_columns, questions_having_subcategories, _i, _j = get_columns_from_formatted_codebook(FILE_PATH_REFORMATED_SOSEC_CODE_BOOK,VERBOSE)\n",
    "\n",
    "# list of column names to keep\n",
    "columns_to_keep =  ['i_TIME'] + required_columns \n",
    "\n",
    "#Only keep the columns as per sosec dataset\n",
    "output_file_path = filter_datasets_for_required_col(FILE_PATH_DATASET,FILE_PATH_REFORMATED_SOSEC_CODE_BOOK,columns_to_keep,VERBOSE)\n",
    "\n",
    "#Check for duplicate columns in the filtered dataset\n",
    "check_duplicate_cols(output_file_path)\n",
    "\n",
    "#replace 6 and 0 in column F2A3 by empty cells\n",
    "output_file_path = perform_filter_on_dataset_F2A3(output_file_path)\n",
    "#Get row count of dataset\n",
    "print(\"\\n No of data in original dataset\")\n",
    "no_of_rows = get_row_count(output_file_path)\n",
    "no_of_columns = count_columns_in_csv(output_file_path)\n",
    "\n",
    "\n",
    "#replace 0 in column F7mA1 (job category) by empty cells\n",
    "output_file_path = perform_filter_on_dataset_F7mA1(output_file_path)\n",
    "#Get row count of dataset\n",
    "print(\"\\n No of data in original dataset\")\n",
    "no_of_rows = get_row_count(output_file_path)\n",
    "no_of_columns = count_columns_in_csv(output_file_path)\n",
    "\n",
    "\n",
    "output_file_path = delete_rows_for_out_of_range_data(output_file_path,FILE_PATH_REFORMATED_SOSEC_CODE_BOOK,False)\n",
    "#Get row count of dataset\n",
    "print(\"\\n No of data after removing out of invalid range values from dataset\")\n",
    "no_of_rows = get_row_count(output_file_path)\n",
    "no_of_columns = count_columns_in_csv(output_file_path)\n",
    "\n",
    "\n",
    "#filter rows based on F7cA1 (YOB) values, remove where out of reasonable age range.\n",
    "output_file_path = perform_filter_on_dataset_F7cA1(output_file_path,1959,2004)\n",
    "#Get row count of datasetc\n",
    "print(\"\\n No of data rows after drop rows having out of range F7cA1 (YOB) values\")\n",
    "no_of_rows = get_row_count(output_file_path)\n",
    "no_of_columns = count_columns_in_csv(output_file_path)\n",
    "\n",
    "#Not implemented as Not relevant also confirmed with Miriam. \n",
    "#source: https://www.census.gov/geographies/reference-files/time-series/geo/gazetteer-files.html\n",
    "#Filter on valid usa zip code\n",
    "#zipcode_file = r\"../data/0_US_zipcodes.csv\"\n",
    "#output_file_path = clean_and_filter_dataset_F7bA1(output_file_path, zipcode_file)\n",
    "#print(\"\\n No of data rows after drop rows where F7bA1 (Zip) is wrong.\")\n",
    "#no_of_rows = get_row_count(output_file_path)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "#Delete columns having more than 70% null with exceptions of some columns\n",
    "col_to_exclude =[\"F6a_DemPartyA2\",\"F6a_RepPartyA2\",\"F6b_DemPartyA2\",\"F6b_RepPartyA2\"]\n",
    "output_file_path = drop_high_null_columns(output_file_path,FILE_PATH_REFORMATED_SOSEC_CODE_BOOK, col_to_exclude,0.7)\n",
    "print(\"\\n No of data rows after Delete columns having more than 70% null with exceptions of some columns\")\n",
    "no_of_rows = get_row_count(output_file_path)\n",
    "no_of_columns = count_columns_in_csv(output_file_path)\n",
    "\n",
    "#Delete data which is filled in rapidly without reading\n",
    "output_file_path = remove_below_percentile(output_file_path,10)\n",
    "print(\"\\n No of data rows which is filled in rapidly without reading using time of i_TIME less then some percentile value\")\n",
    "no_of_rows = get_row_count(output_file_path)\n",
    "no_of_columns = count_columns_in_csv(output_file_path)\n",
    "\n",
    "\n",
    "#Commented and disabled for time being. \n",
    "#Output CSV file size was: 997 MB\n",
    "#No of data rows and columns after encoding and removing the encoded columns.\n",
    "#output Number of rows in the csv: 19875\n",
    "#output Number of columns in the CSV: 8592\n",
    "\n",
    "#Encode the columns which are based on categorical values. \n",
    "#F7a:Gender\n",
    "#F7bA1\tEnter a 5-digit Zip number.:\n",
    "#F7d\tWere you born in the US?\n",
    "#F7e\tWas your mother born in the US?\n",
    "#F7f\tWas your father born in the US?\n",
    "#F7g:  educational level\n",
    "#F7h: employment status?\n",
    "#F7i\tWhat is your marital status?\n",
    "#F7lA1\tWhich religious community do you belong to?\n",
    "#F7mA1\tTo which of the following occupational groups do you belong?\n",
    "#F7n\tWhich ethnic group do you belong to?\n",
    "#input_file_path = output_file_path\n",
    "#columns_to_encode = ['F7a', 'F7bA1','F7d','F7e','F7f','F7g', 'F7h', 'F7i', 'F7lA1', 'F7mA1','F7n']  \n",
    "# Perform one-hot encoding\n",
    "#output_file_path = one_hot_encode_csv(input_file_path, columns_to_encode)\n",
    "#print(\"\\nNo of data rows and columns after encoding and removing the encoded columns.\")\n",
    "#no_of_rows = get_row_count(output_file_path)\n",
    "#no_of_columns = count_columns_in_csv(output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
