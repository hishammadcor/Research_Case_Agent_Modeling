{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_specific_files(file_paths):\n",
    "    \"\"\"\n",
    "    Deletes specific files given their paths.\n",
    "\n",
    "    Parameters:\n",
    "        file_paths (list): A list of file paths to delete.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the status of each file.\n",
    "              Format: {file_path: \"Deleted\" or \"Error: <reason>\"}\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            if os.path.exists(file_path):\n",
    "                os.remove(file_path)\n",
    "                result[file_path] = \"Deleted\"\n",
    "            else:\n",
    "                result[file_path] = \"Error: File does not exist\"\n",
    "        except Exception as e:\n",
    "            result[file_path] = f\"Error: {str(e)}\"\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_column_in_csv(input_file, output_file, column_to_drop):\n",
    "    \"\"\"\n",
    "    Drops a specific column from a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        input_file (str): Path to the input CSV file.\n",
    "        output_file (str): Path to the output CSV file with the column removed.\n",
    "        column_to_drop (str): The name of the column to be dropped.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the input CSV\n",
    "        with open(input_file, 'r', newline='', encoding='utf-8') as infile:\n",
    "            reader = csv.DictReader(infile)\n",
    "            fieldnames = [col for col in reader.fieldnames if col != column_to_drop]\n",
    "            \n",
    "            if column_to_drop not in reader.fieldnames:\n",
    "                print(f\"Column '{column_to_drop}' not found in the CSV.\")\n",
    "                return\n",
    "\n",
    "            # Write to the output CSV\n",
    "            with open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "                writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "                for row in reader:\n",
    "                    del row[column_to_drop]\n",
    "                    writer.writerow(row)\n",
    "\n",
    "        print(f\"Column '{column_to_drop}' has been removed and saved to '{output_file}'.\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{input_file}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicate_cols(file_path):\n",
    "    '''\n",
    "    Input: csv file path\n",
    "    Checks if the csv file have duplicate column names.\n",
    "    '''\n",
    "    df = pd.read_csv(file_path,encoding=\"utf-8\")\n",
    "\n",
    "    # Get column names and find duplicates\n",
    "    column_names = df.columns\n",
    "    duplicate_columns = column_names[column_names.duplicated()].tolist()\n",
    "\n",
    "    if duplicate_columns:\n",
    "        print(f\"WARNING! Duplicate column titles found: {duplicate_columns}\")\n",
    "    else:\n",
    "        print(\"\\nNice! No duplicate column titles found.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns_from_formatted_codebook(file_path_formatted_codebook, verbose):\n",
    "    '''\n",
    "    input: formatted codebook filepath\n",
    "    Read the formatted codebook csv to get the questions and their color categories\n",
    "    '''\n",
    "    # Read the dataset from file\n",
    "    df = pd.read_csv(file_path_formatted_codebook)\n",
    "\n",
    "    # Creating no_color_questions for rows where 'color' is empty\n",
    "    no_color_questions = df.loc[df['Color_Category'].isnull(), 'Custom_variable_name'].tolist()\n",
    "\n",
    "    # Creating lists for each color, where grey_questions=question having sub categories.\n",
    "    grey_questions  = df.loc[df['Color_Category'] == 'Grey', 'Custom_variable_name'].tolist()\n",
    "    yellow_questions = df.loc[df['Color_Category'] == 'Yellow', 'Custom_variable_name'].tolist()\n",
    "    green_questions  = df.loc[df['Color_Category'] == 'Green', 'Custom_variable_name'].tolist()\n",
    "    \n",
    "    question_with_characteristics= df.loc[df['Characteristic'] != '-', 'Custom_variable_name'].tolist()\n",
    "\n",
    "    # Output the results if needed\n",
    "    if(verbose):\n",
    "        print(\"::::get_columns_from_formatted_codebook:::::Returns\")\n",
    "        print(\"no_color_questions:\", no_color_questions)\n",
    "        print(\"grey_questions(question having sub categories):\", grey_questions )\n",
    "        print(\"yellow_questions:\", yellow_questions )\n",
    "        print(\"green_questions:\", green_questions )\n",
    "        print(\"green_questions:\", green_questions )\n",
    "    return no_color_questions, grey_questions,yellow_questions, green_questions,question_with_characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_datasets_for_required_col(file_path_dataset,file_path_formatted_sosec_code_book, columns_to_keep, verbose=False):\n",
    "    '''\n",
    "    input:codebook file path and sosec dataset file path\n",
    "    Only Keeps the columns in the dataset which are also in the codebook and saves the files\n",
    "    also Saves a file for the columns in the code book which are not in the dataset\n",
    "    '''\n",
    "    # Read the dataset CSV file\n",
    "    df = pd.read_csv(file_path_dataset)\n",
    "\n",
    "    # Check which columns are not found\n",
    "    not_found_columns = [col for col in columns_to_keep if col not in df.columns]\n",
    "\n",
    "    if not_found_columns:\n",
    "        # Read the formatted_sosec_code_book CSV file\n",
    "        file_path_2 = file_path_formatted_sosec_code_book\n",
    "        df2 = pd.read_csv(file_path_2,encoding=\"utf-8\")\n",
    "        \n",
    "        # Filter rows where Custom_variable_name matches values in not_found_columns\n",
    "        df_no_matches = df2[df2['Custom_variable_name'].isin(not_found_columns)]\n",
    "        \n",
    "        # Select the corresponding Text values\n",
    "        result_df = df_no_matches[['Custom_variable_name', 'Text']]\n",
    "        \n",
    "        # Save the result as a new CSV\n",
    "        output_file_path = r'../data/1_preprocess/1_codebook_no_matching_columns_in_dataset.csv'\n",
    "        result_df.to_csv(output_file_path, index=False)\n",
    "        if(verbose):\n",
    "            print(f\"Not Matched columns saved to: {output_file_path}\")\n",
    "\n",
    "    # Filter the DataFrame to keep only the columns that exist in the DataFrame\n",
    "    df_filtered = df[[col for col in columns_to_keep if col in df.columns]]\n",
    "    \n",
    "    output_file_path = r'../data/1_preprocess/1_df_dataset_with_codebook_columns_full_no_processing.csv'\n",
    "    df_filtered.to_csv(output_file_path, index=False)\n",
    "\n",
    "    if(verbose):\n",
    "        print(df_filtered.head())\n",
    "    \n",
    "    return output_file_path+\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row_count(file_path_csv):\n",
    "    '''\n",
    "    input filepath of csv\n",
    "    Returns the number of rows in the given CSV file.\n",
    "    '''\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path_csv)\n",
    "\n",
    "    # Get the number of rows\n",
    "    row_count = len(df)\n",
    "\n",
    "    print(f\"Number of rows in the csv: {row_count}\")\n",
    "    return row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_columns_in_csv(file_path_csv):\n",
    "    \"\"\"\n",
    "    Counts and returns the number of columns in a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): The path to the CSV file.\n",
    "    \n",
    "    Returns:\n",
    "    int: The number of columns in the CSV file.\n",
    "    \"\"\"\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path_csv)\n",
    "    \n",
    "    # Get the number of col\n",
    "    columns_count = len(df.columns)\n",
    "    \n",
    "    \n",
    "    print(f\"Number of columns in the CSV: {columns_count}\")\n",
    "    # Return the number of columns\n",
    "    return columns_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Excluded\n",
    "\n",
    "\n",
    "def perform_filter_on_dataset_F2A3(file_path_dataset_with_codebook_columns):\n",
    "    '''\n",
    "    input the datafile path with only codebook columns\n",
    "    replace 6 and 0 by blank\n",
    "    F2A3 = The possibility of losing your job (leave this empty if you do not work)\n",
    "    '''\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path_dataset_with_codebook_columns)\n",
    "\n",
    "    # Replace 6 and 0 in the 'F2A3' column with  empty\n",
    "    df['F2A3'] = df['F2A3'].replace({6: \"\", 0: \"\"})\n",
    "\n",
    "    # Save the updated DataFrame back to a CSV file\n",
    "    output_file_path = r'../data/1_preprocess/2_df_dataset_with_codebook_columns_full_F2A3.csv'\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Updated dataset saved to: {output_file_path}\")\n",
    "    return output_file_path+\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Excluded\n",
    "\n",
    "def perform_filter_on_dataset_F7mA1(file_path_dataset_with_codebook_columns):\n",
    "    '''\n",
    "    input the datafile path with only codebook columns\n",
    "    replace 0 by blank\n",
    "    F7mA1 = job category (leave this empty if you do not work)\n",
    "    '''\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path_dataset_with_codebook_columns)\n",
    "\n",
    "    # Replace 0 in the 'F7mA1' column with  empty\n",
    "    df['F7mA1'] = df['F7mA1'].replace({0: \"\"})\n",
    "\n",
    "    # Save the updated DataFrame back to a CSV file\n",
    "    output_file_path = r'../data/1_preprocess/3_df_dataset_with_codebook_columns_full_F7mA1.csv'\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Updated dataset saved to: {output_file_path}\")\n",
    "    return output_file_path+\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_rows_for_out_of_range_data(sosec_data_path,file_path_dataset_with_codebook_columns,verbose=False):\n",
    "    '''\n",
    "    input: sosec_datafile path and sosec dataset with only codebook columns\n",
    "    Deletes the rows for all the columns for with the values are out of range. and saves the csv\n",
    "    '''\n",
    "    file1 = pd.read_csv(sosec_data_path)\n",
    "\n",
    "    file2 = pd.read_csv(file_path_dataset_with_codebook_columns)\n",
    "\n",
    "    # Create a dictionary from file2 with ranges\n",
    "    ranges = {}\n",
    "    for _, row in file2.iterrows():\n",
    "        col = row['Custom_variable_name']\n",
    "        range_str = row['Characteristic']\n",
    "        \n",
    "        # Check if range_str is a valid string before splitting\n",
    "        if isinstance(range_str, str) and range_str and range_str != '-':  # Valid range and not 'F'\n",
    "            # If there's a valid range, split it into a list of integers\n",
    "            ranges[col] = list(map(int, range_str.split(',')))\n",
    "\n",
    "        else:\n",
    "            # If no valid range is provided (empty or 'F'), set the range to None\n",
    "            ranges[col] = None\n",
    "\n",
    "    # Validate the data in file1 against the ranges\n",
    "    def validate_data(file1, ranges):\n",
    "        errors = []\n",
    "        valid_rows = file1.copy()  # Copy of the original DataFrame to modify\n",
    "        \n",
    "        # Loop through each column and validate values\n",
    "        for col in file1.columns:\n",
    "            if col in ranges:\n",
    "                valid_range = ranges[col]\n",
    "                if valid_range is not None:  # Only check if a valid range exists\n",
    "                    # Create a boolean mask for invalid rows\n",
    "                    invalid_rows = ~(valid_rows[col].isin(valid_range) | valid_rows[col].isna())      \n",
    "\n",
    "                    # Track errors for rows with out-of-range data\n",
    "                    for index, value in valid_rows[invalid_rows][col].dropna().items():\n",
    "                        errors.append(f\"Out of range: {col} at row {index + 1} with value {value}\")\n",
    "                    \n",
    "                    # Remove rows with invalid data\n",
    "                    valid_rows = valid_rows[~invalid_rows]\n",
    "                else:\n",
    "                    # If no range is provided, assume all values are valid for that column\n",
    "                    continue\n",
    "        \n",
    "        return valid_rows, errors\n",
    "\n",
    "    # Get valid rows and errors\n",
    "    valid_rows, errors = validate_data(file1, ranges)\n",
    "\n",
    "    # Save the valid rows to a CSV file\n",
    "    output_file_path = r\"../data/1_preprocess/4_df_dataset_with_codebook_columns_filtered_outofrange.csv\"\n",
    "    valid_rows.to_csv(output_file_path, index=False)\n",
    "\n",
    "    if(verbose):\n",
    "        # Output validation errors\n",
    "        if errors:\n",
    "            print(\"Validation Errors:\")\n",
    "            for error in errors:\n",
    "                print(error)\n",
    "        else:\n",
    "            print(\"All data is within valid ranges.\")\n",
    "\n",
    "    # Print where the cleaned data has been saved\n",
    "    print(f\"Cleaned data saved to '{output_file_path}'.\")\n",
    "\n",
    "    return output_file_path+\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_filter_on_dataset_F7cA1(file_path_dataset_with_codebook_columns,min_yob,max_yob):\n",
    "    '''\n",
    "    filter rows based on range for F7cA1(Yob).\n",
    "    '''\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path_dataset_with_codebook_columns)\n",
    "\n",
    "    # Filter out rows where 'F7cA1' is not in range\n",
    "    df = df[(df['F7cA1'] >= min_yob) & (df['F7cA1'] <= max_yob)]\n",
    "\n",
    "    # Save the updated DataFrame back to a CSV file\n",
    "    output_file_path = r'../data/1_preprocess/5_df_dataset_with_codebook_columns_filtered_F7cA1-yob.csv'\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Updated dataset saved to: {output_file_path}\")\n",
    "    return output_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_codefile_YOB_char_and_label(file_path, custom_variable_name, yob_start, yob_end, output_file):\n",
    "    \"\"\"\n",
    "    Updates the Characteristic and Value_labels in the CSV file for a specific Custom_variable_name\n",
    "    and saves the updated data to a new file.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "        custom_variable_name (str): The Custom_variable_name to search for.\n",
    "        yob_start (int): Start year for Characteristic.\n",
    "        yob_end (int): End year for Characteristic.\n",
    "        output_file (str): Path to save the updated CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    updated_rows = []\n",
    "    updated_characteristic = \", \".join(str(year) for year in range(yob_start, yob_end + 1))\n",
    "    updated_value_labels = f\"{yob_start} to {yob_end}\"\n",
    "\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        with open(file_path, 'r', newline='', encoding='utf-8') as infile:\n",
    "            reader = csv.DictReader(infile)\n",
    "            fieldnames = reader.fieldnames\n",
    "            \n",
    "            # Ensure the required columns exist\n",
    "            if \"Custom_variable_name\" not in fieldnames or \"Characteristic\" not in fieldnames or \"Value_labels\" not in fieldnames:\n",
    "                print(\"Error: Required columns (Custom_variable_name, Characteristic, Value_labels) are missing in the CSV.\")\n",
    "                return\n",
    "\n",
    "            # Process each row\n",
    "            for row in reader:\n",
    "                if row[\"Custom_variable_name\"] == custom_variable_name:\n",
    "                    row[\"Characteristic\"] = updated_characteristic\n",
    "                    row[\"Value_labels\"] = updated_value_labels\n",
    "                updated_rows.append(row)\n",
    "\n",
    "        # Write to the new output CSV file\n",
    "        with open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(updated_rows)\n",
    "\n",
    "        print(f\"Updated CSV file has been saved as '{output_file}'.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_high_null_columns(FILE_PATH_DATASET,file_path_formatted_sosec_code_book ,col_to_exclude, threshold=0.7 ):\n",
    "    '''\n",
    "    Deletes columns with more than 70% null values from file_name_A, excluding from list of columns and saves the dataset.\n",
    "    Prints removed columns and saves them to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        file_name_A (str): Path to the input CSV file.\n",
    "        col_to_exclude(list): Name of the column to exclude from deletion.\n",
    "        threshold (float): Proportion of nulls above which columns are dropped.\n",
    "        output_file (str): Path to save the filtered dataset.\n",
    "        removed_columns_file (str): Path to save the list of removed columns.\n",
    "    '''\n",
    "\n",
    "    output_file = r\"../data/1_preprocess/6_df_dataset_with_codebook_columns_filtered_lessdata.csv\"\n",
    "\n",
    "    # Load the dataset\n",
    "    df_A = pd.read_csv(FILE_PATH_DATASET)\n",
    "    \n",
    "    # Calculate the threshold for null values\n",
    "    null_threshold = threshold * len(df_A)\n",
    "    \n",
    "    # Identify columns to keep based on null percentage and exceptions\n",
    "    cols_to_keep = [col for col in df_A.columns \n",
    "                    if (df_A[col].isna().sum() <= null_threshold) or col in col_to_exclude]\n",
    "    \n",
    "    # Identify the columns to remove\n",
    "    cols_to_remove = [col for col in df_A.columns if col not in cols_to_keep]\n",
    "    \n",
    "    # Print the removed columns\n",
    "    print(f\"Removed columns: {cols_to_remove}\")\n",
    "    \n",
    "\n",
    "    if cols_to_remove:\n",
    "        # Read the formatted_sosec_code_book CSV file\n",
    "        file_path_2 = file_path_formatted_sosec_code_book\n",
    "        df2 = pd.read_csv(file_path_2,encoding=\"utf-8\")\n",
    "        \n",
    "        # Filter rows where Custom_variable_name matches values in not_found_columns\n",
    "        df_no_matches = df2[df2['Custom_variable_name'].isin(cols_to_remove)]\n",
    "        \n",
    "        # Select the corresponding Text values\n",
    "        result_df = df_no_matches[['Custom_variable_name', 'Text']]\n",
    "        \n",
    "        # Save the result as a new CSV\n",
    "        removed_columns_file= r\"../data/1_preprocess/6_removed_columns_due_to_lessdata.csv\"\n",
    "        result_df.to_csv(removed_columns_file, index=False)\n",
    "    \n",
    "        print(f\"Not Matched columns saved to: {removed_columns_file}\")\n",
    "\n",
    "\n",
    "    # Filter the DataFrame to keep only the selected columns\n",
    "    df_filtered = df_A[cols_to_keep]\n",
    "\n",
    "    # Save the filtered dataset\n",
    "    df_filtered.to_csv(output_file, index=False)\n",
    "    print(f\"Filtered dataset saved to: {output_file}\")\n",
    "    print(f\"List of removed columns saved to: {removed_columns_file}\")\n",
    "\n",
    "    return output_file+\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_below_percentile(file_path_sosec_dataset, percentile=10):\n",
    "    '''\n",
    "    performs filter on i_TIME using the given percentile value\n",
    "    input: csv of sosec dataset.\n",
    "    percentile value\n",
    "    output: filtered dataset as csv\n",
    "    '''\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path_sosec_dataset)\n",
    "\n",
    "    # Calculate the 10th percentile of the 'i_TIME' column\n",
    "    percentile_value = df['i_TIME'].quantile(percentile / 100.0)\n",
    "\n",
    "    # Filter the DataFrame to keep only rows where 'i_TIME' is greater than or equal to the 10th percentile\n",
    "    df_filtered = df[df['i_TIME'] >= percentile_value]\n",
    "\n",
    "    # Save the updated DataFrame back to a CSV file\n",
    "    output_file_path = r\"../data/1_preprocess/7_df_dataset_with_codebook_columns_filtered_itime.csv\"\n",
    "    df_filtered.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Updated dataset saved to: {output_file_path}\")\n",
    "    return output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load CSV and one-hot encode specific columns\n",
    "def one_hot_encode_csv(input_file_path, columns_to_encode):\n",
    "    '''\n",
    "    input: csv of sosec dataset.\n",
    "    list of columns to encode.\n",
    "    deleted the columns_to_encode from the data.\n",
    "    save the csv\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(input_file_path)\n",
    "    \n",
    "    # Select only available columns from the list\n",
    "    available_columns = [col for col in columns_to_encode if col in df.columns]\n",
    "\n",
    "    # One-hot encode the available columns\n",
    "    df_encoded = pd.get_dummies(df, columns=available_columns)\n",
    "\n",
    "    output_file_path = r\"../data/1_preprocess/8_df_dataset_with_codebook_columns_filtered_hotencoding.csv\"\n",
    "\n",
    "    # Save the encoded DataFrame to a new CSV file\n",
    "    df_encoded.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"One-hot encoded CSV saved to {output_file_path}\")\n",
    "    return output_file_path+\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_all_csv_files(folder_path):\n",
    "    \"\"\"\n",
    "    Deletes all CSV files in the specified folder.\n",
    "\n",
    "    Parameters:\n",
    "        folder_path (str): Relative or absolute path to the folder.\n",
    "    \"\"\"\n",
    "    # Get the full path of all CSV files in the folder\n",
    "    csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "    if not (csv_files):\n",
    "        print(f\"No csv files to delete in {folder_path}\")\n",
    "    else:\n",
    "        for file_path in csv_files:\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted: {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete {file_path}: {e}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_personas_format1(FILE_PATH_DATASET,FILE_PATH_REFORMATED_SOSEC_CODE_BOOK):\n",
    "    # Load the files\n",
    "    file1_path = FILE_PATH_REFORMATED_SOSEC_CODE_BOOK\n",
    "    file2_path = FILE_PATH_DATASET\n",
    "\n",
    "    file1 = pd.read_csv(file1_path)\n",
    "    file2 = pd.read_csv(file2_path)\n",
    "\n",
    "    # Get the column names from file2 (excluding i_TIME)\n",
    "    file2.rename(columns={\"i_TIME\": \"i_TIME (Time taken in seconds to fill the survey)\"}, inplace=True)\n",
    "    file2_columns = file2.columns[0:]\n",
    "\n",
    "    # Check which columns in file2 exist in the Custom_variable_name of file1\n",
    "    common_columns = file1[file1['Custom_variable_name'].isin(file2_columns)]\n",
    "\n",
    "    # Extract the required values\n",
    "    text_row = common_columns.set_index('Custom_variable_name').reindex(file2_columns)['Text'].fillna('').values.tolist()\n",
    "    characteristic_row = common_columns.set_index('Custom_variable_name').reindex(file2_columns)['Characteristic'].fillna('').values.tolist()\n",
    "    value_labels_row = common_columns.set_index('Custom_variable_name').reindex(file2_columns)['Value_labels'].fillna('').values.tolist()\n",
    "\n",
    "    # Insert these rows into file2\n",
    "    file2 = pd.DataFrame(file2) \n",
    "    new_rows = pd.DataFrame([text_row, characteristic_row, value_labels_row], columns=file2.columns)\n",
    "\n",
    "    # Combine the new rows with the original data\n",
    "    file2 = pd.concat([new_rows, file2], ignore_index=True)\n",
    "\n",
    "    # Save the updated file2 to a new CSV\n",
    "    FILE_PATH_PERSONA_FORMAT1 = r\"../data/1_preprocess/9_processed_data_for_personas_Format_1.csv\"\n",
    "    file2.to_csv(FILE_PATH_PERSONA_FORMAT1, index=False)\n",
    "\n",
    "    print(f\"Updated file2 saved to {FILE_PATH_PERSONA_FORMAT1}\")\n",
    "    return FILE_PATH_PERSONA_FORMAT1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_personas_format2(FILE_PATH_PERSONA_FORMAT1):\n",
    "    # Load the files\n",
    "    df= pd.read_csv(FILE_PATH_PERSONA_FORMAT1, header=None) \n",
    "\n",
    "    # Save the updated file1\n",
    "    output_file_path = r\"../data/1_preprocess/9_processed_data_for_personas_Format_2.csv\"\n",
    "\n",
    "    # Remove the first row (the old header row)\n",
    "    df = df.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "    # Save the updated DataFrame back to a CSV file without the header\n",
    "    df.to_csv(output_file_path, header=False, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: ../data/1_preprocess\\1_codebook_no_matching_columns_in_dataset.csv\n",
      "Deleted: ../data/1_preprocess\\6_removed_columns_due_to_lessdata.csv\n",
      "Deleted: ../data/1_preprocess\\9_processed_data_for_personas_Format_1.csv\n",
      "Deleted: ../data/1_preprocess\\9_processed_data_for_personas_Format_2.csv\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jim\\AppData\\Local\\Temp\\ipykernel_60252\\368653022.py:8: DtypeWarning: Columns (57,58,62,68,75,76) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path_dataset)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nice! No duplicate column titles found.\n",
      "\n",
      "Cleaned data saved to '../data/1_preprocess/4_df_dataset_with_codebook_columns_filtered_outofrange.csv'.\n",
      "\n",
      "No of data after removing out of invalid range values from dataset\n",
      "Number of rows in the csv: 24149\n",
      "Number of columns in the CSV: 133\n",
      "Updated dataset saved to: ../data/1_preprocess/5_df_dataset_with_codebook_columns_filtered_F7cA1-yob.csv\n",
      "\n",
      "No of data rows after drop rows having out of range F7cA1 (YOB:1959-2004) values\n",
      "Number of rows in the csv: 15197\n",
      "Number of columns in the CSV: 133\n",
      "Updated CSV file has been saved as '../data/temp_SOSEC_Code-book.csv'.\n",
      "Removed columns: ['F3A23_1', 'F3A24_1', 'F3A25_1', 'F3A26_1', 'F3A27_1', 'F3A28_1', 'F3A29_1', 'F3A30_1', 'F3A31_1', 'F3A32_1', 'F3A33_1', 'F3A34_1', 'F3A35_1', 'F3A36_1', 'F3B1', 'F3B2', 'F3B3', 'F3B_Hamas', 'F3B_Israel', 'F3B_Deutschland', 'F3B_USA', 'F3B_UN', 'F3B_Iran', 'F3B_Katar', 'F5A14_1', 'F5A15_1', 'F5bA2_1', 'F5cA1_1', 'F5cA2_1']\n",
      "Not Matched columns saved to: ../data/1_preprocess/6_removed_columns_due_to_lessdata.csv\n",
      "Filtered dataset saved to: ../data/1_preprocess/6_df_dataset_with_codebook_columns_filtered_lessdata.csv\n",
      "List of removed columns saved to: ../data/1_preprocess/6_removed_columns_due_to_lessdata.csv\n",
      "\n",
      "No of data rows after Delete columns having more than 70% null with exceptions of columns: ['F6a_DemPartyA2', 'F6a_RepPartyA2', 'F6b_DemPartyA2', 'F6b_RepPartyA2'] \n",
      "Number of rows in the csv: 15197\n",
      "Number of columns in the CSV: 104\n",
      "Updated dataset saved to: ../data/1_preprocess/7_df_dataset_with_codebook_columns_filtered_itime.csv\n",
      "\n",
      "No of data rows and col which is filled having time of i_TIME more then 10th percentile\n",
      "Number of rows in the csv: 13691\n",
      "Number of columns in the CSV: 104\n",
      "Column 'i_TIME' has been removed and saved to '../data/1_preprocess/7a_df_dataset_with_codebook_columns_filtered_itime.csv'.\n",
      "Updated file2 saved to ../data/1_preprocess/9_processed_data_for_personas_Format_1.csv\n",
      "../data/1_preprocess/1_df_dataset_with_codebook_columns_full_no_processing.csv: Deleted\n",
      "../data/1_preprocess/2_df_dataset_with_codebook_columns_full_F2A3.csv: Error: File does not exist\n",
      "../data/1_preprocess/3_df_dataset_with_codebook_columns_full_F7mA1.csv: Error: File does not exist\n",
      "../data/1_preprocess/4_df_dataset_with_codebook_columns_filtered_outofrange.csv: Deleted\n",
      "../data/1_preprocess/5_df_dataset_with_codebook_columns_filtered_F7cA1-yob.csv: Deleted\n",
      "../data/1_preprocess/6_df_dataset_with_codebook_columns_filtered_lessdata.csv: Deleted\n",
      "../data/1_preprocess/7_df_dataset_with_codebook_columns_filtered_itime.csv: Deleted\n",
      "../data/1_preprocess/7a_df_dataset_with_codebook_columns_filtered_itime.csv: Deleted\n",
      "../data/temp_SOSEC_Code-book.csv: Deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jim\\AppData\\Local\\Temp\\ipykernel_60252\\1231411800.py:3: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df= pd.read_csv(FILE_PATH_PERSONA_FORMAT1, header=None)\n"
     ]
    }
   ],
   "source": [
    "# Load the required data files.\n",
    "FILE_PATH_DATASET = r\"..\\data\\0_SOSEC Data RCS\\data_sample_35_SOSEC_dataset_us.csv\"\n",
    "FILE_PATH_REFORMATED_SOSEC_CODE_BOOK = r'..\\data\\0_Reformated_SOSEC_Code-book_US_November_Reformulated_Questions_For_Dict_All_Columns.csv' \n",
    "VERBOSE = False\n",
    "\n",
    "# Call the function with the relative path to delete all csv files in 1_preprocess folder.\n",
    "delete_all_csv_files(r\"../data/1_preprocess/\")\n",
    "\n",
    "#Get list of columns from formatted sosec code book\n",
    "#no_color_questions, grey_questions,yellow_questions, green_questions,question_with_characteristics\n",
    "_i, _j, _k, _l,required_columns = get_columns_from_formatted_codebook(FILE_PATH_REFORMATED_SOSEC_CODE_BOOK,VERBOSE)\n",
    "\n",
    "# list of column names to keep\n",
    "columns_to_keep =  ['i_TIME'] + required_columns \n",
    "\n",
    "#Only keep the columns as per sosec dataset\n",
    "output_file_path = filter_datasets_for_required_col(FILE_PATH_DATASET,FILE_PATH_REFORMATED_SOSEC_CODE_BOOK,columns_to_keep,VERBOSE)\n",
    "\n",
    "#Check for duplicate columns in the filtered dataset\n",
    "check_duplicate_cols(output_file_path)\n",
    "\n",
    "#Changed the 0 to 6 in the codebook.\n",
    "#replace 6 and 0 in column F2A3 by empty cells\n",
    "#output_file_path = perform_filter_on_dataset_F2A3(output_file_path)\n",
    "#print(\"\\nNo of rows and columns in original dataset after setting 6,0 to empty in F2A3\")\n",
    "#no_of_rows = get_row_count(output_file_path)\n",
    "#no_of_columns = count_columns_in_csv(output_file_path)\n",
    "\n",
    "#replace 0 in column F7mA1 (job category) by empty cells\n",
    "#output_file_path = perform_filter_on_dataset_F7mA1(output_file_path)\n",
    "#print(\"\\nNo of rows and columns in original dataset after setting 0 to empty in F7mA1\")\n",
    "#no_of_rows = get_row_count(output_file_path)\n",
    "#no_of_columns = count_columns_in_csv(output_file_path)\n",
    "\n",
    "#Remove out of range data\n",
    "output_file_path = delete_rows_for_out_of_range_data(output_file_path,FILE_PATH_REFORMATED_SOSEC_CODE_BOOK,False)\n",
    "print(\"\\nNo of data after removing out of invalid range values from dataset\")\n",
    "no_of_rows = get_row_count(output_file_path)\n",
    "no_of_columns = count_columns_in_csv(output_file_path)\n",
    "\n",
    "\n",
    "#filter rows based on F7cA1 (YOB) values, remove where out of reasonable age range.\n",
    "yob_start = 1959\n",
    "yob_end = 2004\n",
    "output_file_path = perform_filter_on_dataset_F7cA1(output_file_path,1959,2004)\n",
    "print(f\"\\nNo of data rows after drop rows having out of range F7cA1 (YOB:{yob_start}-{yob_end}) values\")\n",
    "no_of_rows = get_row_count(output_file_path)\n",
    "no_of_columns = count_columns_in_csv(output_file_path)\n",
    "\n",
    "\n",
    "\n",
    "#Correct YOB range in code book for later use as temp file\n",
    "file_path = r\"../data/0_Reformated_SOSEC_Code-book_US_November_Reformulated_Questions_For_Dict_All_Columns.csv\"\n",
    "SOSEC_Codebook_path = r\"../data/temp_SOSEC_Code-book.csv\"\n",
    "custom_variable_name = \"F7cA1\"\n",
    "yob_start = 1959\n",
    "yob_end = 2004\n",
    "update_codefile_YOB_char_and_label(file_path, custom_variable_name, yob_start, yob_end, SOSEC_Codebook_path)\n",
    "\n",
    "\n",
    "FILE_PATH_REFORMATED_SOSEC_CODE_BOOK = SOSEC_Codebook_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Delete columns having more than 70% null with exceptions of some columns\n",
    "col_to_exclude =[\"F6a_DemPartyA2\",\"F6a_RepPartyA2\",\"F6b_DemPartyA2\",\"F6b_RepPartyA2\"]\n",
    "output_file_path = drop_high_null_columns(output_file_path,FILE_PATH_REFORMATED_SOSEC_CODE_BOOK, col_to_exclude,0.7)\n",
    "print(f\"\\nNo of data rows after Delete columns having more than 70% null with exceptions of columns: {col_to_exclude} \")\n",
    "no_of_rows = get_row_count(output_file_path)\n",
    "no_of_columns = count_columns_in_csv(output_file_path)\n",
    "\n",
    "#Delete data which is filled in rapidly without reading by using i_time column\n",
    "percentile_val = 10\n",
    "output_file_path = remove_below_percentile(output_file_path,10)\n",
    "print(f\"\\nNo of data rows and col which is filled having time of i_TIME more then {percentile_val}th percentile\")\n",
    "no_of_rows = get_row_count(output_file_path)\n",
    "no_of_columns = count_columns_in_csv(output_file_path)\n",
    "\n",
    "#Drop i_time\n",
    "input_csv=output_file_path\n",
    "output_file_path=r\"../data/1_preprocess/7a_df_dataset_with_codebook_columns_filtered_itime.csv\"\n",
    "column_to_remove='i_TIME'\n",
    "drop_column_in_csv(input_csv, output_file_path, column_to_remove)\n",
    "\n",
    "\n",
    "#Add info as rows for creating personas.\n",
    "FILE_PATH_PERSONA_FORMAT1= prepare_for_personas_format1(output_file_path,FILE_PATH_REFORMATED_SOSEC_CODE_BOOK)\n",
    "\n",
    "\n",
    "#Rename all first row of row labels.\n",
    "prepare_for_personas_format2(FILE_PATH_PERSONA_FORMAT1)\n",
    "\n",
    "\n",
    "#Delete generated extra files.\n",
    "file_paths = [\n",
    "    #r\"../data/1_preprocess/1_codebook_no_matching_columns_in_dataset.csv\",\n",
    "    r\"../data/1_preprocess/1_df_dataset_with_codebook_columns_full_no_processing.csv\",\n",
    "    r\"../data/1_preprocess/2_df_dataset_with_codebook_columns_full_F2A3.csv\",\n",
    "    r\"../data/1_preprocess/3_df_dataset_with_codebook_columns_full_F7mA1.csv\",\n",
    "    r\"../data/1_preprocess/4_df_dataset_with_codebook_columns_filtered_outofrange.csv\",\n",
    "    r\"../data/1_preprocess/5_df_dataset_with_codebook_columns_filtered_F7cA1-yob.csv\",\n",
    "    r\"../data/1_preprocess/6_df_dataset_with_codebook_columns_filtered_lessdata.csv\",\n",
    "    #r\"../data/1_preprocess/6_removed_columns_due_to_lessdata.csv\",\n",
    "    r\"../data/1_preprocess/7_df_dataset_with_codebook_columns_filtered_itime.csv\",\n",
    "    r\"../data/1_preprocess/7a_df_dataset_with_codebook_columns_filtered_itime.csv\",\n",
    "    r\"../data/temp_SOSEC_Code-book.csv\"\n",
    "]\n",
    "\n",
    "status = delete_specific_files(file_paths)\n",
    "for file, message in status.items():\n",
    "    print(f\"{file}: {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Encode the columns which are based on categorical values. \\n#F7a:Gender\\n#F7bA1\\tEnter a 5-digit Zip number.:\\n#F7d\\tWere you born in the US?\\n#F7e\\tWas your mother born in the US?\\n#F7f\\tWas your father born in the US?\\n#F7g:  educational level\\n#F7h: employment status?\\n#F7i\\tWhat is your marital status?\\n#F7lA1\\tWhich religious community do you belong to?\\n#F7mA1\\tTo which of the following occupational groups do you belong?\\n#F7n\\tWhich ethnic group do you belong to?\\ninput_file_path = output_file_path\\ncolumns_to_encode = [\\'F7a\\', \\'F7bA1\\',\\'F7d\\',\\'F7e\\',\\'F7f\\',\\'F7g\\', \\'F7h\\', \\'F7i\\', \\'F7lA1\\', \\'F7mA1\\',\\'F7n\\']  \\n# Perform one-hot encoding\\noutput_file_path = one_hot_encode_csv(input_file_path, columns_to_encode)\\nprint(\"\\nNo of data rows and columns after encoding and removing the encoded columns.\")\\nno_of_rows = get_row_count(output_file_path)\\nno_of_columns = count_columns_in_csv(output_file_path)\\n'"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Not applied: hot-encooding.\n",
    "'''\n",
    "#Encode the columns which are based on categorical values. \n",
    "#F7a:Gender\n",
    "#F7bA1\tEnter a 5-digit Zip number.:\n",
    "#F7d\tWere you born in the US?\n",
    "#F7e\tWas your mother born in the US?\n",
    "#F7f\tWas your father born in the US?\n",
    "#F7g:  educational level\n",
    "#F7h: employment status?\n",
    "#F7i\tWhat is your marital status?\n",
    "#F7lA1\tWhich religious community do you belong to?\n",
    "#F7mA1\tTo which of the following occupational groups do you belong?\n",
    "#F7n\tWhich ethnic group do you belong to?\n",
    "input_file_path = output_file_path\n",
    "columns_to_encode = ['F7a', 'F7bA1','F7d','F7e','F7f','F7g', 'F7h', 'F7i', 'F7lA1', 'F7mA1','F7n']  \n",
    "# Perform one-hot encoding\n",
    "output_file_path = one_hot_encode_csv(input_file_path, columns_to_encode)\n",
    "print(\"\\nNo of data rows and columns after encoding and removing the encoded columns.\")\n",
    "no_of_rows = get_row_count(output_file_path)\n",
    "no_of_columns = count_columns_in_csv(output_file_path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
